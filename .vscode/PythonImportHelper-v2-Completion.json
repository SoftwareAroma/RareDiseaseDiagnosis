[
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "obonet",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "obonet",
        "description": "obonet",
        "detail": "obonet",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "project_config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "project_config",
        "description": "project_config",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "importPath": "simulate_patients.utils",
        "description": "simulate_patients.utils",
        "isExtraImport": true,
        "detail": "simulate_patients.utils",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "igraph",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "igraph",
        "description": "igraph",
        "detail": "igraph",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "preprocess",
        "description": "preprocess",
        "detail": "preprocess",
        "documentation": {}
    },
    {
        "label": "write_patients",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "read_patients",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "read_dicts",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "read_simulated_patients",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "write_patients",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "read_patients",
        "importPath": "project_utils",
        "description": "project_utils",
        "isExtraImport": true,
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "codecs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "codecs",
        "description": "codecs",
        "detail": "codecs",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "snap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "snap",
        "description": "snap",
        "detail": "snap",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "jsonlines",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jsonlines",
        "description": "jsonlines",
        "detail": "jsonlines",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "combinations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "cm",
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "isExtraImport": true,
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pytorch_lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytorch_lightning",
        "description": "pytorch_lightning",
        "detail": "pytorch_lightning",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "WandbLogger",
        "importPath": "pytorch_lightning.loggers",
        "description": "pytorch_lightning.loggers",
        "isExtraImport": true,
        "detail": "pytorch_lightning.loggers",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "rankdata",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "rankdata",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "rankdata",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "rankdata",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "CosineAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "BilinearAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "AdditiveAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "DotProductAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "CosineAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "BilinearAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "AdditiveAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "DotProductAttention",
        "importPath": "allennlp.modules.attention",
        "description": "allennlp.modules.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention",
        "documentation": {}
    },
    {
        "label": "MultisimilarityCriterion",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "_construct_labels",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "unique",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "_construct_disease_labels",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "NCALoss",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "MultisimilarityCriterion",
        "importPath": "utils.loss_utils",
        "description": "utils.loss_utils",
        "isExtraImport": true,
        "detail": "utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "masked_mean",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "masked_softmax",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "weighted_sum",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_degree_vs_attention",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mean_reciprocal_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "top_k_acc",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mean_reciprocal_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "top_k_acc",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "masked_mean",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "masked_softmax",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "weighted_sum",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mean_reciprocal_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "top_k_acc",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "average_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "fit_umap",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mrr_vs_percent_overlap",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_x_intrain",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_hops",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_degree_vs_attention",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_nhops_to_gene_vs_attention",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_fraction_phenotype",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_numtrain",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_trainset",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mean_reciprocal_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "top_k_acc",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "average_rank",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "fit_umap",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_softmax",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mrr_vs_percent_overlap",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_x_intrain",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_hops",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_degree_vs_attention",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_nhops_to_gene_vs_attention",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_fraction_phenotype",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_numtrain",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_trainset",
        "importPath": "utils.train_utils",
        "description": "utils.train_utils",
        "isExtraImport": true,
        "detail": "utils.train_utils",
        "documentation": {}
    },
    {
        "label": "LpDistance",
        "importPath": "pytorch_metric_learning.distances",
        "description": "pytorch_metric_learning.distances",
        "isExtraImport": true,
        "detail": "pytorch_metric_learning.distances",
        "documentation": {}
    },
    {
        "label": "common_functions",
        "importPath": "pytorch_metric_learning.utils",
        "description": "pytorch_metric_learning.utils",
        "isExtraImport": true,
        "detail": "pytorch_metric_learning.utils",
        "documentation": {}
    },
    {
        "label": "loss_and_miner_utils",
        "importPath": "pytorch_metric_learning.utils",
        "description": "pytorch_metric_learning.utils",
        "isExtraImport": true,
        "detail": "pytorch_metric_learning.utils",
        "documentation": {}
    },
    {
        "label": "BaseMetricLossFunction",
        "importPath": "pytorch_metric_learning.losses",
        "description": "pytorch_metric_learning.losses",
        "isExtraImport": true,
        "detail": "pytorch_metric_learning.losses",
        "documentation": {}
    },
    {
        "label": "torch,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.",
        "description": "torch.",
        "detail": "torch.",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "plotly.express",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.express",
        "description": "plotly.express",
        "detail": "plotly.express",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sigmoid",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "average_precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn.parameter",
        "description": "torch.nn.parameter",
        "isExtraImport": true,
        "detail": "torch.nn.parameter",
        "documentation": {}
    },
    {
        "label": "Attention",
        "importPath": "allennlp.modules.attention.attention",
        "description": "allennlp.modules.attention.attention",
        "isExtraImport": true,
        "detail": "allennlp.modules.attention.attention",
        "documentation": {}
    },
    {
        "label": "Activation",
        "importPath": "allennlp.nn",
        "description": "allennlp.nn",
        "isExtraImport": true,
        "detail": "allennlp.nn",
        "documentation": {}
    },
    {
        "label": "umap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "umap",
        "description": "umap",
        "detail": "umap",
        "documentation": {}
    },
    {
        "label": "PdfPages",
        "importPath": "matplotlib.backends.backend_pdf",
        "description": "matplotlib.backends.backend_pdf",
        "isExtraImport": true,
        "detail": "matplotlib.backends.backend_pdf",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "random_split",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SubsetRandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "NodeEmbeder",
        "importPath": "node_embedder_model",
        "description": "node_embedder_model",
        "isExtraImport": true,
        "detail": "node_embedder_model",
        "documentation": {}
    },
    {
        "label": "NodeEmbeder",
        "importPath": "node_embedder_model",
        "description": "node_embedder_model",
        "isExtraImport": true,
        "detail": "node_embedder_model",
        "documentation": {}
    },
    {
        "label": "NodeEmbeder",
        "importPath": "node_embedder_model",
        "description": "node_embedder_model",
        "isExtraImport": true,
        "detail": "node_embedder_model",
        "documentation": {}
    },
    {
        "label": "GPAligner",
        "importPath": "task_heads.gp_aligner",
        "description": "task_heads.gp_aligner",
        "isExtraImport": true,
        "detail": "task_heads.gp_aligner",
        "documentation": {}
    },
    {
        "label": "GPAligner",
        "importPath": "task_heads.gp_aligner",
        "description": "task_heads.gp_aligner",
        "isExtraImport": true,
        "detail": "task_heads.gp_aligner",
        "documentation": {}
    },
    {
        "label": "get_edges",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "calc_metrics",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "sample_node_for_et",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_batched_data",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_edges",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "calc_metrics",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "plot_roc_curve",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "metrics_per_rel",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_edges",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "calc_metrics",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_indices_into_edge_index",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "HeterogeneousEdgeIndex",
        "importPath": "utils.pretrain_utils",
        "description": "utils.pretrain_utils",
        "isExtraImport": true,
        "detail": "utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "BatchNorm",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GATv2Conv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "bilinear",
        "importPath": "decoders",
        "description": "decoders",
        "isExtraImport": true,
        "detail": "decoders",
        "documentation": {}
    },
    {
        "label": "trans",
        "importPath": "decoders",
        "description": "decoders",
        "isExtraImport": true,
        "detail": "decoders",
        "documentation": {}
    },
    {
        "label": "dot",
        "importPath": "decoders",
        "description": "decoders",
        "isExtraImport": true,
        "detail": "decoders",
        "documentation": {}
    },
    {
        "label": "PatientNCA",
        "importPath": "shepherd.task_heads.patient_nca",
        "description": "shepherd.task_heads.patient_nca",
        "isExtraImport": true,
        "detail": "shepherd.task_heads.patient_nca",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "PatientDataset",
        "importPath": "shepherd.dataset",
        "description": "shepherd.dataset",
        "isExtraImport": true,
        "detail": "shepherd.dataset",
        "documentation": {}
    },
    {
        "label": "PatientDataset",
        "importPath": "shepherd.dataset",
        "description": "shepherd.dataset",
        "isExtraImport": true,
        "detail": "shepherd.dataset",
        "documentation": {}
    },
    {
        "label": "PatientNeighborSampler",
        "importPath": "shepherd.samplers",
        "description": "shepherd.samplers",
        "isExtraImport": true,
        "detail": "shepherd.samplers",
        "documentation": {}
    },
    {
        "label": "PatientNeighborSampler",
        "importPath": "shepherd.samplers",
        "description": "shepherd.samplers",
        "isExtraImport": true,
        "detail": "shepherd.samplers",
        "documentation": {}
    },
    {
        "label": "get_predict_hparams",
        "importPath": "hparams",
        "description": "hparams",
        "isExtraImport": true,
        "detail": "hparams",
        "documentation": {}
    },
    {
        "label": "get_pretrain_hparams",
        "importPath": "hparams",
        "description": "hparams",
        "isExtraImport": true,
        "detail": "hparams",
        "documentation": {}
    },
    {
        "label": "get_pretrain_hparams",
        "importPath": "hparams",
        "description": "hparams",
        "isExtraImport": true,
        "detail": "hparams",
        "documentation": {}
    },
    {
        "label": "get_train_hparams",
        "importPath": "hparams",
        "description": "hparams",
        "isExtraImport": true,
        "detail": "hparams",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "train",
        "description": "train",
        "isExtraImport": true,
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "load_patient_datasets",
        "importPath": "train",
        "description": "train",
        "isExtraImport": true,
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "get_dataloaders",
        "importPath": "train",
        "description": "train",
        "isExtraImport": true,
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "LearningRateMonitor",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "importPath": "torch_geometric.data.sampler",
        "description": "torch_geometric.data.sampler",
        "isExtraImport": true,
        "detail": "torch_geometric.data.sampler",
        "documentation": {}
    },
    {
        "label": "EdgeIndex",
        "importPath": "torch_geometric.data.sampler",
        "description": "torch_geometric.data.sampler",
        "isExtraImport": true,
        "detail": "torch_geometric.data.sampler",
        "documentation": {}
    },
    {
        "label": "Adj",
        "importPath": "torch_geometric.data.sampler",
        "description": "torch_geometric.data.sampler",
        "isExtraImport": true,
        "detail": "torch_geometric.data.sampler",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "importPath": "samplers",
        "description": "samplers",
        "isExtraImport": true,
        "detail": "samplers",
        "documentation": {}
    },
    {
        "label": "SparseTensor",
        "importPath": "torch_sparse",
        "description": "torch_sparse",
        "isExtraImport": true,
        "detail": "torch_sparse",
        "documentation": {}
    },
    {
        "label": "random_walk",
        "importPath": "torch_cluster",
        "description": "torch_cluster",
        "isExtraImport": true,
        "detail": "torch_cluster",
        "documentation": {}
    },
    {
        "label": "pad_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "add_self_loops",
        "importPath": "torch_geometric.utils",
        "description": "torch_geometric.utils",
        "isExtraImport": true,
        "detail": "torch_geometric.utils",
        "documentation": {}
    },
    {
        "label": "add_remaining_self_loops",
        "importPath": "torch_geometric.utils",
        "description": "torch_geometric.utils",
        "isExtraImport": true,
        "detail": "torch_geometric.utils",
        "documentation": {}
    },
    {
        "label": "negative_sampling",
        "importPath": "torch_geometric.utils",
        "description": "torch_geometric.utils",
        "isExtraImport": true,
        "detail": "torch_geometric.utils",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "label_binarize",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "to_networkx",
        "importPath": "torch_geometric.utils.convert",
        "description": "torch_geometric.utils.convert",
        "isExtraImport": true,
        "detail": "torch_geometric.utils.convert",
        "documentation": {}
    },
    {
        "label": "to_scipy_sparse_matrix",
        "importPath": "torch_geometric.utils.convert",
        "description": "torch_geometric.utils.convert",
        "isExtraImport": true,
        "detail": "torch_geometric.utils.convert",
        "documentation": {}
    },
    {
        "label": "torch.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.multiprocessing",
        "description": "torch.multiprocessing",
        "detail": "torch.multiprocessing",
        "documentation": {}
    },
    {
        "label": "CombinedGPAligner",
        "importPath": "shepherd.gene_prioritization_model",
        "description": "shepherd.gene_prioritization_model",
        "isExtraImport": true,
        "detail": "shepherd.gene_prioritization_model",
        "documentation": {}
    },
    {
        "label": "CombinedPatientNCA",
        "importPath": "shepherd.patient_nca_model",
        "description": "shepherd.patient_nca_model",
        "isExtraImport": true,
        "detail": "shepherd.patient_nca_model",
        "documentation": {}
    },
    {
        "label": "faulthandler;",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faulthandler;",
        "description": "faulthandler;",
        "detail": "faulthandler;",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "find_missing_genes",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def find_missing_genes(kg_nodes, orpha_genes, use_ensembl=False):\n    gene_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'gene/protein']\n    if not use_ensembl:\n        # manually curated mapping from aliases to gene symbol name using genecards.org\n        orpha_genes = set([gene_symbol_alias_dict[g] if g in gene_symbol_alias_dict else g for g in orpha_genes])\n    genes_in_kg = set(gene_nodes['node_name'].tolist())\n    orphanet_genes_missing_in_kg = set(orpha_genes).difference(genes_in_kg)\n    ensembl_str = 'ensembl' if use_ensembl else ''\n    print(f'There are {len(orphanet_genes_missing_in_kg)} orphanet {ensembl_str} genes missing in the KG of {len(set(orpha_genes))} genes')\n    return orphanet_genes_missing_in_kg, genes_in_kg",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "find_missing_phenotypes",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def find_missing_phenotypes(kg_nodes, orpha_phenotypes):\n    # phenotypes\n    phenotype_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'effect/phenotype']\n    HPO_LEN = 7\n    padding_needed = HPO_LEN - phenotype_nodes['node_id'].str.len()\n    padded_hpo = padding_needed.apply(lambda x: 'HP:' + '0' * x)\n    phenotype_nodes['hpo_string'] = padded_hpo + phenotype_nodes['node_id'] \n    orphanet_phenotypes_missing_in_kg = set(orpha_phenotypes).difference(set(phenotype_nodes['hpo_string'].tolist()))\n    print(f'There are {len(orphanet_phenotypes_missing_in_kg)} orphanet phenotypes missing in the KG of {len(set(orpha_phenotypes))} phenotypes')\n    return orphanet_phenotypes_missing_in_kg, phenotype_nodes",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "find_missing_diseases",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def find_missing_diseases(kg_nodes, orpha_diseases, orphanet_to_mondo_dict, mondo_definitions_obo_map, mondo_to_hpo_dict, phenotype_nodes):\n    # get mapping from MONDO disease to KG idx\n    disease_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'disease']\n    disease_nodes['node_id'] = disease_nodes['node_id'].str.replace('.0', '', regex=False)\n    disease_to_idx_dict = {str(mondo_str):idx + 1 for mondo, idx in zip(disease_nodes['node_id'].tolist(), disease_nodes['node_index'].tolist()) for mondo_str in mondo.split('_')}\n    # get mapping from phenotypes to KG idx\n    phenotype_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'effect/phenotype']\n    phen_to_idx_dict = {int(phen):idx + 1 for phen, idx in zip(phenotype_nodes['node_id'].tolist(), phenotype_nodes['node_index'].tolist()) if int(phen) in mondo_to_hpo_dict.values()}\n    disease_mapped_phen_to_idx_dict = {str(mondo):phen_to_idx_dict[hpo] for mondo,hpo in mondo_to_hpo_dict.items()}\n    # merge two mappings",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "map_to_ensembl",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def map_to_ensembl(kg_nodes):\n    if (project_config.KG_DIR / 'raw' / 'nodes_with_ensembl.csv').exists():\n        kg_nodes = pd.read_csv(project_config.KG_DIR / 'raw' / 'nodes_with_ensembl.csv')\n        kg_nodes.columns = ['node_index','node_id','node_type','node_name','node_source','old_node_name']\n    else:\n        preprocessor = preprocess.Preprocessor()\n        # get gene nodes & map to ensembl IDs\n        gene_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'gene/protein']\n        gene_nodes = preprocessor.map_genes(gene_nodes, ['node_name'])\n        print('Done mapping genes')",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "find_missing_disease_gene_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def find_missing_disease_gene_edges(orpha_disease_gene_edges, kg, kg_nodes, orphanet_to_mondo_dict, orphanet_genes_missing_in_kg, diseases_missing_in_kg, mondo_to_hpo_dict ):\n    ## get edges from orphanet\n    # map gene names to their alias\n    orpha_disease_gene_edges = [(d, gene_symbol_alias_dict[g]) if g in gene_symbol_alias_dict else (d,g) for (d,g) in list(orpha_disease_gene_edges)]\n    # convert orphanet orpha to gene edges into mondo to gene edges\n    orpha_disease_gene_edges = [(orphanet_to_mondo_dict[int(d)], g)  for (d,g) in orpha_disease_gene_edges if int(d) in orphanet_to_mondo_dict]\n    orpha_disease_gene_edges = [(d, g) for d_list, g in orpha_disease_gene_edges for d in d_list]\n    ## get edges from KG\n    gene_disease_edges = kg.loc[(kg['x_type'] == 'gene/protein') & (kg['y_type'] == 'disease')]\n    disease_gene_edges = kg.loc[(kg['x_type'] == 'disease') & (kg['y_type'] == 'gene/protein')] # reverse relations",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "find_missing_disease_phenotype_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def find_missing_disease_phenotype_edges(orpha_disease_phenotype_edges, kg, kg_nodes, orphanet_to_mondo_dict, orphanet_phenotypes_missing_in_kg, diseases_missing_in_kg, mondo_to_hpo_dict):\n    # disease phenotype edges from orphanet\n    orpha_disease_phenotype_edges = [(orphanet_to_mondo_dict[int(d)], p)  for (d,p) in orpha_disease_phenotype_edges if int(d) in orphanet_to_mondo_dict]\n    orpha_disease_phenotype_edges = [(d, re.sub('HP:0*', '', p)) for d_list,p in orpha_disease_phenotype_edges for d in d_list]\n    # disease phenotype edges from the KG\n    phen_disease_edges = kg.loc[(kg['x_type'] == 'effect/phenotype') & (kg['y_type'] == 'disease')]\n    disease_phen_edges = kg.loc[(kg['x_type'] == 'disease') & (kg['y_type'] == 'effect/phenotype')] # reverse relations\n    assert len(phen_disease_edges.index) == 0\n    kg_disease_phen_edges_tup = list(zip(disease_phen_edges['x_id'], disease_phen_edges['y_id']))\n    kg_disease_phen_edges_tup = [(d,p) for d_list, p in kg_disease_phen_edges_tup for d in d_list.split('_')]",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "clean_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def clean_edges(df): \n    df = df.dropna()\n    df = df.drop_duplicates()\n    df = df.query('not ((x_id == y_id) and (x_type == y_type) and (x_source == y_source) and (x_name == y_name))')\n    return df\ndef main():\n    # Read in Orphanet Data\n    orphanet_phenotypes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_hpo_normalized.tsv', sep='\\t', dtype=str)\n    orphanet_genes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_genes.tsv', sep='\\t', dtype=str)\n    orphanet_ensembl_genes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_genes_normalized.tsv', sep='\\t', dtype=str)",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "def main():\n    # Read in Orphanet Data\n    orphanet_phenotypes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_hpo_normalized.tsv', sep='\\t', dtype=str)\n    orphanet_genes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_genes.tsv', sep='\\t', dtype=str)\n    orphanet_ensembl_genes = pd.read_csv(project_config.UDN_DATA / 'orphanet' / 'flat_files_2019-10-29' / 'orphanet_final_disease_genes_normalized.tsv', sep='\\t', dtype=str)\n    # read in mapping from old to current phenotypes\n    hp_terms = pd.read_csv(project_config.KG_DIR / 'raw' / 'sources' / 'hpo' / 'hp_terms.csv')\n    hp_map_dict = {'HP:' + ('0' * (7-len(str(int(hp_old))))) + str(int(hp_old)): 'HP:' + '0' * (7-len(str(int(hp_new)))) + str(int(hp_new)) for hp_old,hp_new in zip(hp_terms['id'], hp_terms['replacement_id'] ) if not pd.isnull(hp_new)}\n    orphanet_phenotypes.replace({\"HPO_ID\": hp_map_dict}, inplace=True)\n    orpha_phenotypes = orphanet_phenotypes['HPO_ID'].unique().tolist()",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "pd.options.mode.chained_assignment",
        "kind": 5,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "pd.options.mode.chained_assignment = None\nimport sys\nimport obonet\nimport re\nsys.path.insert(0, '..') # add config to path\nsys.path.insert(0, '../../udn_knowledge_graph/') # add config to path\nimport project_config\nfrom simulate_patients.utils import preprocess\n'''\nThere are some relationships found in Orphanet that are missing from the KG. We add them here. ",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "gene_symbol_alias_dict",
        "kind": 5,
        "importPath": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "description": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "peekOfCode": "gene_symbol_alias_dict = {'RARS':'RARS1', 'C11ORF80':'C11orf80', 'KIF1BP':'KIFBP', 'ICK':'CILK1', 'AARS':'AARS1', 'SPG23':'DSTYK', \n        'C9ORF72':'C9orf72', 'C8ORF37':'C8orf37', 'HARS':'HARS1', 'GARS':'GARS1', 'C19ORF12':'C19orf12', \n        'C12ORF57':'C12orf57', 'ADSSL1':'ADSS1', 'C12ORF65':'C12orf65', 'MARS':'MARS1', 'CXORF56':'STEEP1', \n        'SARS':'SARS1', 'C12ORF4':'C12orf4', 'MUT':'MMUT', 'LOR':'LORICRIN'}\ndef find_missing_genes(kg_nodes, orpha_genes, use_ensembl=False):\n    gene_nodes = kg_nodes.loc[kg_nodes['node_type'] == 'gene/protein']\n    if not use_ensembl:\n        # manually curated mapping from aliases to gene symbol name using genecards.org\n        orpha_genes = set([gene_symbol_alias_dict[g] if g in gene_symbol_alias_dict else g for g in orpha_genes])\n    genes_in_kg = set(gene_nodes['node_name'].tolist())",
        "detail": "data_prep.construct_kg.add_orphanet_data_to_kg",
        "documentation": {}
    },
    {
        "label": "clean_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def clean_edges(df): \n    df = df.get(['relation', 'display_relation', 'x_id', 'x_idx', 'x_type', 'x_name', 'x_source', 'y_id', 'y_idx', 'y_type', 'y_name', 'y_source'])\n    assert len(df[df.isna().any(axis=1)]) == 0\n    df = df.dropna()\n    df = df.drop_duplicates()\n    df = df.query('not ((x_id == y_id) and (x_idx == y_idx) and (x_type == y_type) and (x_source == y_source) and (x_name == y_name))')\n    return df\ndef get_node_df(graph): # Assign nodes to IDs between 0 and N-1\n    # Get all nodes\n    nodes = pd.concat([graph.get(['x_id','x_type', 'x_name','x_source']).rename(columns={'x_id':'node_id', 'x_type':'node_type', 'x_name':'node_name','x_source':'node_source'}), graph.get(['y_id','y_type', 'y_name','y_source']).rename(columns={'y_id':'node_id', 'y_type':'node_type', 'y_name':'node_name','y_source':'node_source'})]).drop_duplicates(ignore_index=True)",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "get_node_df",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def get_node_df(graph): # Assign nodes to IDs between 0 and N-1\n    # Get all nodes\n    nodes = pd.concat([graph.get(['x_id','x_type', 'x_name','x_source']).rename(columns={'x_id':'node_id', 'x_type':'node_type', 'x_name':'node_name','x_source':'node_source'}), graph.get(['y_id','y_type', 'y_name','y_source']).rename(columns={'y_id':'node_id', 'y_type':'node_type', 'y_name':'node_name','y_source':'node_source'})]).drop_duplicates(ignore_index=True)\n    # Assign them to 0 to N-1\n    nodes = nodes.reset_index().drop('index',axis=1).reset_index().rename(columns={'index':'node_idx'})\n    print(nodes)\n    print(\"Finished assigning all nodes to IDs between 0 to N-1\")\n    return nodes\ndef reindex_edges(graph, nodes): # Assign node indices to nodes in edge\n    # Map source nodes",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "reindex_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def reindex_edges(graph, nodes): # Assign node indices to nodes in edge\n    # Map source nodes\n    edges = pd.merge(graph, nodes, 'left', left_on=['x_id', 'x_type', 'x_name','x_source'], right_on=['node_id','node_type','node_name','node_source'])\n    edges = edges.rename(columns={'node_idx':'x_idx'})\n    # Map target nodes\n    edges = pd.merge(edges, nodes, 'left', left_on=['y_id','y_type', 'y_name','y_source'], right_on=['node_id','node_type','node_name','node_source'])\n    edges = edges.rename(columns={'node_idx':'y_idx'})\n    # Subset only node info\n    edges = edges.get(['x_idx', 'x_type', 'y_idx', 'y_type', 'relation', 'display_relation']).drop_duplicates(ignore_index=True).reset_index()\n    print(edges)",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "split_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def split_edges(edges): # Generate data splits\n    split_idx = list(range(len(edges)))\n    random.shuffle(split_idx)\n    train_idx = split_idx[ : int(len(split_idx) * 0.8)]\n    val_idx = split_idx[int(len(split_idx) * 0.8) : int(len(split_idx) * 0.9)]\n    test_idx = split_idx[int(len(split_idx) * 0.9) : ]\n    assert len(set(train_idx).intersection(set(val_idx), set(test_idx))) == 0\n    mask = np.zeros(len(split_idx))\n    mask[train_idx] = 0\n    mask[val_idx] = 1",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "get_LCC",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def get_LCC(full_graph, nodes):\n    edge_index = full_graph.get(['x_idx', 'y_idx']).values.T\n    graph = ig.Graph()\n    graph.add_vertices(list(range(nodes.shape[0])))\n    graph.add_edges([tuple(x) for x in edge_index.T])\n    graph = graph.as_undirected(mode='collapse')\n    print('Before LCC - Nodes: %d' % graph.vcount())\n    print('Before LCC - Edges: %d' % graph.ecount())\n    c = graph.components(mode='strong')\n    giant = c.giant()",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "map_to_LCC",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def map_to_LCC(full_graph, giant, nodes, giant_nodes):\n    new_nodes = nodes.query('node_idx in @giant_nodes')\n    new_nodes = new_nodes.reset_index().drop('index',axis=1).reset_index().rename(columns={'index':'new_node_idx'})\n    assert new_nodes.shape[0] == giant.vcount()\n    assert len(new_nodes[\"node_idx\"].to_list()) == len(new_nodes[\"new_node_idx\"].to_list())\n    new_edges = full_graph.query('x_idx in @giant_nodes and y_idx in @giant_nodes').copy()\n    new_edges = new_edges.reset_index(drop=True)\n    assert new_edges.shape[0] == giant.ecount()\n    new_kg = pd.merge(new_edges, new_nodes, 'left', left_on='x_idx', right_on='node_idx')\n    new_kg = new_kg.rename(columns={'node_id':'new_x_id', 'node_type':'new_x_type', 'node_name':'new_x_name', 'node_source':'new_x_source', 'new_node_idx':'new_x_idx'})",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "triadic_closure",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def triadic_closure(graph):\n    '''\n    'disease_phenotype_positive' & 'disease_protein' -> 'phenotype_protein' \n    '''\n    print(f'Before triadic closure - Nodes: {len(pd.concat([graph[\"x_idx\"], graph[\"y_idx\"]]).unique())}')\n    print(f'Before triadic closure - Edges: {len(graph[\"relation\"].tolist())}')\n    d_phen_relations = graph.loc[graph['relation'] == 'disease_phenotype_positive']\n    d_prot_relations = graph.loc[graph['relation'] == 'disease_protein']\n    merged_relations = d_phen_relations.set_index(['x_id', 'x_idx', 'x_name', 'x_source', 'x_type']) \\\n        .join(d_prot_relations.set_index(['x_id', 'x_idx', 'x_name', 'x_source', 'x_type']), how='inner', rsuffix='_dp')",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "add_reverse_edges",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def add_reverse_edges(graph):\n    print(graph.columns)\n    rev_edges = graph[[\"x_idx\", \"x_type\", \"relation\", \"y_idx\", \"y_type\", \"mask\"]].copy()\n    print(rev_edges)\n    rev_edges.columns = [\"y_idx\", \"y_type\", \"relation\", \"x_idx\", \"x_type\", \"mask\"]\n    rev_edge_eqtype = rev_edges.query('x_type == y_type')\n    rev_edge_eqtype[\"relation\"] = rev_edge_eqtype[\"relation\"] + \"_rev\"\n    rev_edge_neqtype = rev_edges.query('x_type != y_type')\n    rev_edges = pd.concat((rev_edge_eqtype, rev_edge_neqtype)).drop_duplicates(ignore_index=True).reset_index()\n    print(rev_edges)",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "generate_edgelist",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def generate_edgelist(node_map_f, mask_f, graph, triad_closure):\n    print(\"Starting to process the KG table\")\n    nodes = get_node_df(graph)\n    edges = reindex_edges(graph, nodes)\n    print(\"Starting to generate the connected KG\")\n    giant = get_LCC(edges, nodes)\n    giant_nodes = giant.vs['name']\n    new_kg, new_nodes = map_to_LCC(edges, giant, nodes, giant_nodes)\n    if triad_closure:\n        print('Performing triadic closure on P-G-D relationships.')",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.construct_kg.prepare_graph",
        "description": "data_prep.construct_kg.prepare_graph",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Prepare graph.\")\n    parser.add_argument('--triad_closure', action='store_true', \\\n        help='Whether to add edges between phenotypes & genes if edges exist between P-D and D-G')\n    args = parser.parse_args()\n    graph = pd.read_csv(project_config.KG_DIR / 'kg_giant_orphanet.csv', dtype={\"x_id\": str, \"y_id\": str})\n    filter_list = [\"contraindication\", \"drug_drug\", \"side_effect\", \"drug_targets\", \"drug_protein\", \"drug_effect\", \"indication\", \"off-label use\", \"exposure_protein\", \"exposure_molfunc\", \"exposure_cellcomp\", \"exposure_bioprocess\", \"exposure_disease\", \"exposure_exposure\", \"anatomy_protein_present\", \"anatomy_protein_absent\", \"anatomy_anatomy\", \"protein_present_anatomy\", \"protein_absent_anatomy\"]\n    graph = graph.loc[~graph['relation'].isin(filter_list)]\n    print(graph)\n    graph = graph[graph[\"x_name\"] != \"missing\"]",
        "detail": "data_prep.construct_kg.prepare_graph",
        "documentation": {}
    },
    {
        "label": "concat_patients",
        "kind": 2,
        "importPath": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "description": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "peekOfCode": "def concat_patients():\n    # combine all of the mygene2 patients\n    if not (MYGENE2_FOLDER / 'mygene2_patients.csv').exists():\n        dfs = []\n        for f in MYGENE2_FOLDER.iterdir():\n            if not str(f).endswith('.sh') and not str(f).endswith('.csv'):\n                df = pd.read_csv(f)\n                dfs.append(df)\n        all_df = pd.concat(dfs)\n        all_df.to_csv(MYGENE2_FOLDER / 'mygene2_patients.csv')",
        "detail": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "documentation": {}
    },
    {
        "label": "normalize_patients",
        "kind": 2,
        "importPath": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "description": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "peekOfCode": "def normalize_patients():\n    # Map genes to ensembl IDs\n    if not (MYGENE2_FOLDER / 'mygene2_patients_genes_normalized.csv').exists():\n        preprocessor = preprocess.Preprocessor(hpo_source_year='2019') #TODO: uncomment if need to use\n        all_df = preprocessor.map_genes(all_df, ['Gene Name']) \\\n                .rename(columns={'Gene Name_ensembl':'ensembl_ids', 'Gene Name_mapping_status': 'mapping_status'})\n        gene_to_ensembl = {'SP3A': 'ENSG00000172845', 'GNA01':'ENSG00000087258', 'MUNC13-1':'ENSG00000130477', 'METAZOA_SRP': 'ENSG00000274848'}\n        all_df = all_df.replace({\"ensembl_ids\": gene_to_ensembl})\n        all_df.to_csv(MYGENE2_FOLDER / 'mygene2_patients_genes_normalized.csv')\n    else:",
        "detail": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "documentation": {}
    },
    {
        "label": "convert_to_jsonl",
        "kind": 2,
        "importPath": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "description": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "peekOfCode": "def convert_to_jsonl(phenotypes_df, all_df, omim_to_mondo_map, omim_to_orpha_match, orphanet_to_category_map):\n    n_with_multiple_genes = 0\n    n_with_single_gene_and_omim = 0\n    patients = []\n    profile_ids = []\n    omim_ids = []\n    for profile_id in phenotypes_df['MyGene2 profile'].unique():\n        phenotypes = phenotypes_df.loc[phenotypes_df['MyGene2 profile'] == profile_id, 'Phenotypes'].tolist()\n        genes = all_df.loc[all_df['MyGene2 profile'] == profile_id, 'ensembl_ids'].tolist()\n        omim = all_df.loc[all_df['MyGene2 profile'] == profile_id, 'OMIM'].tolist()",
        "detail": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "description": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "peekOfCode": "def main():\n    with open(str(project_config.PROJECT_DIR / 'omim_to_mondo_dict.pkl'), 'rb') as handle:\n        omim_to_mondo_map = pickle.load(handle)\n    # OMIM to ORPHA\n    orpha_to_omim_df = pd.read_csv(project_config.PROJECT_DIR / 'data' / 'orphanet'/ 'orphanet_to_omim_mapping_df.csv')\n    orpha_to_omim_df_validated = orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Validated']\n    omim_to_orpha_match = defaultdict(list)\n    for omim, orpha in zip(orpha_to_omim_df_validated['External_ID'], orpha_to_omim_df_validated['OrphaNumber']): omim_to_orpha_match[omim].append(orpha)\n    omim_to_orpha_match = {k:np.unique(v) for k,v in omim_to_orpha_match.items()}\n    omim_to_orpha_match[609943] = 1465 # outdated code so had to manually add mapping",
        "detail": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "documentation": {}
    },
    {
        "label": "MYGENE2_FOLDER",
        "kind": 5,
        "importPath": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "description": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "peekOfCode": "MYGENE2_FOLDER = project_config.PROJECT_DIR / 'patients' / 'mygene2_patients'\ndef concat_patients():\n    # combine all of the mygene2 patients\n    if not (MYGENE2_FOLDER / 'mygene2_patients.csv').exists():\n        dfs = []\n        for f in MYGENE2_FOLDER.iterdir():\n            if not str(f).endswith('.sh') and not str(f).endswith('.csv'):\n                df = pd.read_csv(f)\n                dfs.append(df)\n        all_df = pd.concat(dfs)",
        "detail": "data_prep.create_mygene2_cohort.preprocess_mygene2",
        "documentation": {}
    },
    {
        "label": "html_content",
        "kind": 5,
        "importPath": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "description": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "peekOfCode": "html_content = f.read()\nsoup = BeautifulSoup(html_content)\ngenes = []\nfor link in soup.find_all(\"a\"):\n    href = link.get(\"href\")\n    if href:\n        matches = re.findall(r'api/data/export/(.*)', href)\n        for match in matches:\n            genes.append(match)\nprint(f'There were {len(genes)} genes extracted.')",
        "detail": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "documentation": {}
    },
    {
        "label": "soup",
        "kind": 5,
        "importPath": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "description": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "peekOfCode": "soup = BeautifulSoup(html_content)\ngenes = []\nfor link in soup.find_all(\"a\"):\n    href = link.get(\"href\")\n    if href:\n        matches = re.findall(r'api/data/export/(.*)', href)\n        for match in matches:\n            genes.append(match)\nprint(f'There were {len(genes)} genes extracted.')\ngenes = pd.DataFrame({'genes': genes})",
        "detail": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "documentation": {}
    },
    {
        "label": "genes",
        "kind": 5,
        "importPath": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "description": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "peekOfCode": "genes = []\nfor link in soup.find_all(\"a\"):\n    href = link.get(\"href\")\n    if href:\n        matches = re.findall(r'api/data/export/(.*)', href)\n        for match in matches:\n            genes.append(match)\nprint(f'There were {len(genes)} genes extracted.')\ngenes = pd.DataFrame({'genes': genes})\ngenes.to_csv(project_config.PROJECT_DIR / 'patients' / 'mygene2_patients' / 'genes.csv', index=False, header=False)",
        "detail": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "documentation": {}
    },
    {
        "label": "genes",
        "kind": 5,
        "importPath": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "description": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "peekOfCode": "genes = pd.DataFrame({'genes': genes})\ngenes.to_csv(project_config.PROJECT_DIR / 'patients' / 'mygene2_patients' / 'genes.csv', index=False, header=False)",
        "detail": "data_prep.create_mygene2_cohort.retrieve_mygene2",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "data_prep.shortest_paths.add_spl_to_patients",
        "description": "data_prep.shortest_paths.add_spl_to_patients",
        "peekOfCode": "def normalize(x, x_min=1, x_max=6.5): # min & max determined from across simulated patients\n    return 2 * (x-x_min)/(x_max-x_min) - 1\ndef add_spl_info(patients, spl_matrix, hpo_to_idx_dict, ensembl_to_idx_dict , nid_to_spl_dict, min_spl, max_spl, all_gene_idx, agg_type, x_max):\n    max_spl = -1\n    min_spl = 1000\n    avg_spl_matrix = np.zeros((len(patients), len(all_gene_idx)))\n    print('spl_matrix', spl_matrix.shape)\n    spl_indexing = {}\n    for i, patient in enumerate(tqdm(patients)):\n        patient_id = patient['id']",
        "detail": "data_prep.shortest_paths.add_spl_to_patients",
        "documentation": {}
    },
    {
        "label": "add_spl_info",
        "kind": 2,
        "importPath": "data_prep.shortest_paths.add_spl_to_patients",
        "description": "data_prep.shortest_paths.add_spl_to_patients",
        "peekOfCode": "def add_spl_info(patients, spl_matrix, hpo_to_idx_dict, ensembl_to_idx_dict , nid_to_spl_dict, min_spl, max_spl, all_gene_idx, agg_type, x_max):\n    max_spl = -1\n    min_spl = 1000\n    avg_spl_matrix = np.zeros((len(patients), len(all_gene_idx)))\n    print('spl_matrix', spl_matrix.shape)\n    spl_indexing = {}\n    for i, patient in enumerate(tqdm(patients)):\n        patient_id = patient['id']\n        spl_indexing[patient_id] = i\n        hpo_idx = [hpo_to_idx_dict[p] for p in patient['positive_phenotypes'] if p in hpo_to_idx_dict ]",
        "detail": "data_prep.shortest_paths.add_spl_to_patients",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.shortest_paths.add_spl_to_patients",
        "description": "data_prep.shortest_paths.add_spl_to_patients",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Add SPL to patients.\")\n    parser.add_argument(\"--node_map\", type=str, default='KG_node_map.txt', help=\"Path to node map\")\n    parser.add_argument(\"--spl_matrix\", type=str, default='KG_shortest_path_matrix_onlyphenotypes.npy', help=\"Path to shortest path length matrix\")\n    parser.add_argument(\"--agg_type\", type=str, default='mean', help=\"Type of aggregation\")\n    parser.add_argument(\"--save_prefix\", type=str, default=f'disease_split_all_sim_patients_kg_{project_config.CURR_KG}', help=\"Prefix describing the dataset that will be used to describe the SPL output files\")\n    parser.add_argument('--only_test_data', action='store_true', help='Only calculate SPL for the test data. You might want to do this if you have separate runs for many different test datasets and want to generate separate SPL for train/val & testing')\n    parser.add_argument('--only_train_val_data', action='store_true', help='Only calculate SPL for the train/val data. You might want to do this if you have separate runs for many different test datasets and want to generate separate SPL for train/val & testing')\n    args = parser.parse_args()\n    print('Aggregation type: ', args.agg_type)",
        "detail": "data_prep.shortest_paths.add_spl_to_patients",
        "documentation": {}
    },
    {
        "label": "get_shortest_path",
        "kind": 2,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "def get_shortest_path(node_id):\n    NIdToDistH = snap.TIntH()\n    path_len = snap.GetShortPath(snap_graph, int(node_id), NIdToDistH)\n    paths = np.zeros((n_nodes))\n    for dest_node in NIdToDistH: \n        paths[dest_node] = NIdToDistH[dest_node]\n    return paths\nwith multiprocessing.Pool(processes=20) as pool:\n    shortest_paths = pool.map(get_shortest_path, node_ids)\nall_shortest_paths = np.stack(shortest_paths)",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "snap_graph",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "snap_graph = snap.LoadEdgeList(snap.PUNGraph, str(project_config.KG_DIR / 'KG_edgelist_mask.txt'), 0, 1)\nt0 = time.time()\nnode_ids = np.sort([node.GetId() for node in snap_graph.Nodes()])\nn_nodes = len(list(snap_graph.Nodes()))\nprint(f'There are {n_nodes} nodes in the graph')\ndef get_shortest_path(node_id):\n    NIdToDistH = snap.TIntH()\n    path_len = snap.GetShortPath(snap_graph, int(node_id), NIdToDistH)\n    paths = np.zeros((n_nodes))\n    for dest_node in NIdToDistH: ",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "t0",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "t0 = time.time()\nnode_ids = np.sort([node.GetId() for node in snap_graph.Nodes()])\nn_nodes = len(list(snap_graph.Nodes()))\nprint(f'There are {n_nodes} nodes in the graph')\ndef get_shortest_path(node_id):\n    NIdToDistH = snap.TIntH()\n    path_len = snap.GetShortPath(snap_graph, int(node_id), NIdToDistH)\n    paths = np.zeros((n_nodes))\n    for dest_node in NIdToDistH: \n        paths[dest_node] = NIdToDistH[dest_node]",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "node_ids",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "node_ids = np.sort([node.GetId() for node in snap_graph.Nodes()])\nn_nodes = len(list(snap_graph.Nodes()))\nprint(f'There are {n_nodes} nodes in the graph')\ndef get_shortest_path(node_id):\n    NIdToDistH = snap.TIntH()\n    path_len = snap.GetShortPath(snap_graph, int(node_id), NIdToDistH)\n    paths = np.zeros((n_nodes))\n    for dest_node in NIdToDistH: \n        paths[dest_node] = NIdToDistH[dest_node]\n    return paths",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "n_nodes",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "n_nodes = len(list(snap_graph.Nodes()))\nprint(f'There are {n_nodes} nodes in the graph')\ndef get_shortest_path(node_id):\n    NIdToDistH = snap.TIntH()\n    path_len = snap.GetShortPath(snap_graph, int(node_id), NIdToDistH)\n    paths = np.zeros((n_nodes))\n    for dest_node in NIdToDistH: \n        paths[dest_node] = NIdToDistH[dest_node]\n    return paths\nwith multiprocessing.Pool(processes=20) as pool:",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "all_shortest_paths",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "all_shortest_paths = np.stack(shortest_paths)\nprint(all_shortest_paths.shape)\nt1 = time.time()\nprint(f'It took {t1-t0:0.4f}s to calculate the shortest paths')\n# save all shortest paths\nnp.save(project_config.KG_DIR / 'KG_shortest_path_matrix.npy', all_shortest_paths) \n# subset to shortest paths from all nodes to phenotypes\nnode_map = pd.read_csv(project_config.KG_DIR / \"KG_node_map.txt\", sep=\"\\t\")\ndesired_idx = node_map[node_map[\"node_type\"] == \"effect/phenotype\"][\"node_idx\"].tolist()\nall_shortest_paths_to_phens = all_shortest_paths[:, desired_idx]",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "t1",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "t1 = time.time()\nprint(f'It took {t1-t0:0.4f}s to calculate the shortest paths')\n# save all shortest paths\nnp.save(project_config.KG_DIR / 'KG_shortest_path_matrix.npy', all_shortest_paths) \n# subset to shortest paths from all nodes to phenotypes\nnode_map = pd.read_csv(project_config.KG_DIR / \"KG_node_map.txt\", sep=\"\\t\")\ndesired_idx = node_map[node_map[\"node_type\"] == \"effect/phenotype\"][\"node_idx\"].tolist()\nall_shortest_paths_to_phens = all_shortest_paths[:, desired_idx]\nwith open(project_config.KG_DIR / \"KG_shortest_path_matrix_onlyphenotypes.npy\", \"wb\") as f:\n    np.save(f, all_shortest_paths_to_phens)",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "node_map",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "node_map = pd.read_csv(project_config.KG_DIR / \"KG_node_map.txt\", sep=\"\\t\")\ndesired_idx = node_map[node_map[\"node_type\"] == \"effect/phenotype\"][\"node_idx\"].tolist()\nall_shortest_paths_to_phens = all_shortest_paths[:, desired_idx]\nwith open(project_config.KG_DIR / \"KG_shortest_path_matrix_onlyphenotypes.npy\", \"wb\") as f:\n    np.save(f, all_shortest_paths_to_phens)",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "desired_idx",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "desired_idx = node_map[node_map[\"node_type\"] == \"effect/phenotype\"][\"node_idx\"].tolist()\nall_shortest_paths_to_phens = all_shortest_paths[:, desired_idx]\nwith open(project_config.KG_DIR / \"KG_shortest_path_matrix_onlyphenotypes.npy\", \"wb\") as f:\n    np.save(f, all_shortest_paths_to_phens)",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "all_shortest_paths_to_phens",
        "kind": 5,
        "importPath": "data_prep.shortest_paths.shortest_paths",
        "description": "data_prep.shortest_paths.shortest_paths",
        "peekOfCode": "all_shortest_paths_to_phens = all_shortest_paths[:, desired_idx]\nwith open(project_config.KG_DIR / \"KG_shortest_path_matrix_onlyphenotypes.npy\", \"wb\") as f:\n    np.save(f, all_shortest_paths_to_phens)",
        "detail": "data_prep.shortest_paths.shortest_paths",
        "documentation": {}
    },
    {
        "label": "get_linearized_categories",
        "kind": 2,
        "importPath": "data_prep.get_orpha_disease_categories",
        "description": "data_prep.get_orpha_disease_categories",
        "peekOfCode": "def get_linearized_categories():\n    tree = ET.parse(str(ORPHANET_RAW_PATH / 'en_product7_5.8.2022.xml'))\n    root = tree.getroot()\n    disorderlist_linear = root[1]\n    linear_list = []\n    linear_list_cat = []\n    debug_disorder = None\n    for disorder in disorderlist_linear:\n        print('\\n-----')\n        #print('disorder', disorder.tag, disorder.attrib, disorder.tail, disorder.keys(), disorder.items(), disorder.text)",
        "detail": "data_prep.get_orpha_disease_categories",
        "documentation": {}
    },
    {
        "label": "get_annotations",
        "kind": 2,
        "importPath": "data_prep.get_orpha_disease_categories",
        "description": "data_prep.get_orpha_disease_categories",
        "peekOfCode": "def get_annotations(node, all_children):\n    if node.find('ClassificationNodeChildList').attrib['count'] == 0:\n        return all_children\n    else:\n        for node2 in node.find('ClassificationNodeChildList'):\n            #print('Node2', list(node2))\n            disorder = node2.find('Disorder').find('Name').text\n            disorder_id = node2.find('Disorder').find('OrphaCode').text #node2.find('Disorder').attrib['id']\n            disorder_type = node2.find('Disorder').find('DisorderType').find('Name').text\n            #print(f'Disorder {disorder} with ID {disorder_id} of Type {disorder_type}'  )",
        "detail": "data_prep.get_orpha_disease_categories",
        "documentation": {}
    },
    {
        "label": "get_all_categories",
        "kind": 2,
        "importPath": "data_prep.get_orpha_disease_categories",
        "description": "data_prep.get_orpha_disease_categories",
        "peekOfCode": "def get_all_categories():\n    classifications_dir =  ORPHANET_RAW_PATH / 'rare_dx_classifications'\n    orpha_id_to_classes = defaultdict(list)\n    for path in classifications_dir.glob(\"*.xml\"):\n        print('Processing ', path)\n        tree = ET.parse(str(path))\n        root = tree.getroot()\n        classificationlist = list(root[1])\n        for d in classificationlist:\n            print(list(d))",
        "detail": "data_prep.get_orpha_disease_categories",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.get_orpha_disease_categories",
        "description": "data_prep.get_orpha_disease_categories",
        "peekOfCode": "def main():\n    get_all_categories()\n    get_linearized_categories()\nif __name__ == \"__main__\":\n    main()",
        "detail": "data_prep.get_orpha_disease_categories",
        "documentation": {}
    },
    {
        "label": "ORPHANET_RAW_PATH",
        "kind": 5,
        "importPath": "data_prep.get_orpha_disease_categories",
        "description": "data_prep.get_orpha_disease_categories",
        "peekOfCode": "ORPHANET_RAW_PATH = project_config.PROJECT_DIR /'data'/ 'orphanet' / 'raw'\ndef get_linearized_categories():\n    tree = ET.parse(str(ORPHANET_RAW_PATH / 'en_product7_5.8.2022.xml'))\n    root = tree.getroot()\n    disorderlist_linear = root[1]\n    linear_list = []\n    linear_list_cat = []\n    debug_disorder = None\n    for disorder in disorderlist_linear:\n        print('\\n-----')",
        "detail": "data_prep.get_orpha_disease_categories",
        "documentation": {}
    },
    {
        "label": "Preprocessor",
        "kind": 6,
        "importPath": "data_prep.preprocess",
        "description": "data_prep.preprocess",
        "peekOfCode": "class Preprocessor():\n    def write_pkl(self, d, filename):\n        file_path = Path(config.PREPROCESS_PATH / filename)\n        with open(str(file_path), 'wb') as f: \n            pickle.dump(d, f, protocol=pickle.HIGHEST_PROTOCOL)\n    def load_pkl(self, filename):\n        file_path = Path(config.PREPROCESS_PATH / filename)\n        with open(str(file_path), 'rb') as f: \n            return pickle.load(f)\n    def read_hgnc_mappings(self):",
        "detail": "data_prep.preprocess",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def read_data(args):\n    # read in KG nodes\n    node_df = pd.read_csv(project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / args.node_map, sep='\\t')\n    print(f'Unique node sources: {node_df[\"node_source\"].unique()}')\n    print(f'Unique node types: {node_df[\"node_type\"].unique()}')\n    node_type_dict = {idx:node_type for idx, node_type in zip(node_df['node_idx'], node_df['node_type'])}\n    # read in patients\n    sim_patients = read_simulated_patients(args.simulated_path)\n    print(f'Number of sim patients: {len(sim_patients)}')\n    # orphanet metadata",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_networkx_graph",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_networkx_graph(edges):\n    G = nx.MultiDiGraph()\n    edge_index = list(zip(edges['x_idx'], edges['y_idx']))\n    G.add_edges_from(edge_index)\n    return G\n###################################################################\n# create maps from phenotype/gene to the idx in the KG\ndef create_hpo_to_node_idx_dict(node_df, hp_old_new_map):\n    # get HPO nodes\n    hpo_nodes = node_df.loc[node_df['node_type'] == 'effect/phenotype']",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_hpo_to_node_idx_dict",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_hpo_to_node_idx_dict(node_df, hp_old_new_map):\n    # get HPO nodes\n    hpo_nodes = node_df.loc[node_df['node_type'] == 'effect/phenotype']\n    hpo_nodes['node_id'] = hpo_nodes['node_id'].astype(str) \n    # convert HPO id to string version (e.g. 1 -> HP:0000001)\n    HPO_LEN = 7\n    padding_needed = HPO_LEN - hpo_nodes['node_id'].str.len()\n    padded_hpo = padding_needed.apply(lambda x: 'HP:' + '0' * x)\n    hpo_nodes['hpo_string'] = padded_hpo + hpo_nodes['node_id'] \n    # create dict from HPO ID to node index in graph",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_gene_to_node_idx_dict",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_gene_to_node_idx_dict(args, node_df):\n    ensembl_node_map = Path(str(args.node_map).split('.txt')[0]+ '_ensembl_ids.txt')\n    if ensembl_node_map.exists():\n        node_df = pd.read_csv(ensembl_node_map, sep='\\t')\n    else:\n        print('Generating ensembl_ids for KG')\n        preprocessor = preprocess.Preprocessor() #NOTE: raw data to perform preprocessing is missing from dataverse, but we provide the already processed files for our KG\n        # get gene nodes & map to ensembl IDs\n        gene_nodes = node_df.loc[node_df['node_type'] == 'gene/protein']\n        gene_nodes = preprocessor.map_genes(gene_nodes, ['node_name'])",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_mondo_to_node_idx_dict",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_mondo_to_node_idx_dict(node_df, mondo_to_hpo_dict):\n    '''create mondo disease to node_idx map'''\n    # get disease nodes\n    disease_nodes = node_df.loc[(node_df['node_type'] == 'disease')]\n    disease_nodes['node_id'] = disease_nodes['node_id'].str.replace('.0', '', regex=False)\n    mondo_strs = [str(mondo_str) for mondo, idx in zip(disease_nodes['node_id'].tolist(), disease_nodes['node_idx'].tolist()) for mondo_str in mondo.split('_')]\n    assert len(mondo_strs) == len(list(set(mondo_strs))), 'The following dict may overwrite some mappings if because there are duplicates in the mondo ids'\n    mondo_to_idx_dict = {str(mondo_str):idx for mondo, idx in zip(disease_nodes['node_id'].tolist(), disease_nodes['node_idx'].tolist()) for mondo_str in mondo.split('_')}\n    # get mapping from phenotypes to KG idx\n    phenotype_nodes = node_df.loc[node_df['node_type'] == 'effect/phenotype']",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "map_diseases_to_orphanet",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def map_diseases_to_orphanet(node_df, mondo_orphanet_map):\n    all_orphanet_ids = []\n    for node_id, node_type  in zip(node_df['node_id'], node_df['node_type']):\n        if node_type == 'disease':\n            mondo_ids = node_id.split('_')\n            orphanet_ids = [mondo_orphanet_map[m] for m in mondo_ids if m in mondo_orphanet_map]\n            orphanet_ids = [str(o) for l in orphanet_ids for o in l]\n            #if len(orphanet_ids) > 0: print(mondo_ids, '_'.join(orphanet_ids))\n            if len(orphanet_ids) == 0: all_orphanet_ids.append(None)\n            else: all_orphanet_ids.append('_'.join(orphanet_ids)) #NOTE: some nodes that contain grouped MONDO ids are mapped to multiple orphanet ids",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "filter_patients",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def filter_patients(patients, hpo_to_idx_dict, ensembl_to_idx_dict):\n    '''\n    Filter patients out of the dataset if their causal gene, all of their distractor genes, or all of \n    their phenotypes cannot be found in the KG.\n    '''\n    print(f'Number of patients pre-filtering: {len(patients)}')\n    filtered_patients = [p for p in patients if len(set(p['true_genes']).intersection(set(ensembl_to_idx_dict.keys()))) > 0]\n    print(f'Number of patients after filtering out those with no causal gene in the KG: {len(filtered_patients)}')\n    if 'distractor_genes' in filtered_patients[0]:\n        filtered_patients = [p for p in filtered_patients if len(set(p['distractor_genes']).intersection(set(ensembl_to_idx_dict.keys()))) > 0]",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_dataset_split_from_lists",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_dataset_split_from_lists(filtered_patients, train_list_f, val_list_f):\n    train_list = pd.read_csv(project_config.PROJECT_DIR / 'formatted_patients' / train_list_f, index_col=0)['ids'].tolist()\n    val_list = pd.read_csv(project_config.PROJECT_DIR / 'formatted_patients' / val_list_f, index_col=0)['ids'].tolist()\n    train_patients, val_patients, unsorted_patients = [], [], []\n    for patient in filtered_patients:\n        if patient['id'] in train_list: rand_train_patients.append(patient)\n        elif patient['id'] in val_list: rand_val_patients.append(patient)\n        else: unsorted_patients.append(patient)\n    print(f'There are {len(train_patients)} patients in the train set and {len(val_patients)} in the val set.')\n    print(f'There are {len(unsorted_patients)} unsorted patients.')",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "create_disease_split_dataset",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def create_disease_split_dataset(filtered_patients, frac_train=0.7, frac_val_test=0.15):\n    # divide patients by disease ID into train/val/test\n    diseases = list(set([p['disease_id'] for p in filtered_patients]))\n    n_train = round(len(diseases) * frac_train)\n    n_val_test = round(len(diseases) * frac_val_test)\n    dx_train_patients = diseases[0:n_train]\n    dx_val_patients = diseases[n_train:n_val_test+n_train]\n    dx_test_patients = diseases[n_val_test+n_train:]\n    print('Split of diseases into train/val/test: ', len(dx_train_patients), len(dx_val_patients), len(dx_test_patients))\n    dx_split_train_patients = [p for p in filtered_patients if p['disease_id'] in dx_train_patients]",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Preprocessing Patients & KG.\")\n    parser.add_argument(\"-edgelist\", type=str, default=f'KG_edgelist_mask.txt', help=\"File with edge list\")\n    parser.add_argument(\"-node_map\", type=str, default=f'KG_node_map.txt', help=\"File with node list\")\n    parser.add_argument(\"-simulated_path\", type=str, default=f'{project_config.PROJECT_DIR}/patients/simulated_patients/simulated_patients_formatted.jsonl', help=\"Path to simulated patients\")\n    parser.add_argument(\"-split_dataset\", action='store_true', help=\"Split patient datasets into train/val/test.\")\n    parser.add_argument(\"-split_dataset_from_lists\", action='store_true', help='Whether the train/val/test split IDs should be read from file.')\n    args = parser.parse_args()\n    ## read in data, normalize genes to ensembl ids, and create maps from genes/phenotypes to node idx\n    node_df, node_type_dict, sim_patients, orphanet_metadata, mondo_orphanet_map, orphanet_mondo_map, hp_map_dict, mondo_to_hpo_dict = read_data(args)",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "pd.options.mode.chained_assignment",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "pd.options.mode.chained_assignment = None\n# input locations\nORPHANET_METADATA_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_final_disease_metadata.tsv')\nMONDO_MAP_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo_references.csv')\nMONDO_OBO_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo.obo')\nHP_TERMS = project_config.PROJECT_DIR  / 'preprocess' / 'hp_terms.csv'\nMONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "ORPHANET_METADATA_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "ORPHANET_METADATA_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_final_disease_metadata.tsv')\nMONDO_MAP_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo_references.csv')\nMONDO_OBO_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo.obo')\nHP_TERMS = project_config.PROJECT_DIR  / 'preprocess' / 'hp_terms.csv'\nMONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "MONDO_MAP_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "MONDO_MAP_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo_references.csv')\nMONDO_OBO_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo.obo')\nHP_TERMS = project_config.PROJECT_DIR  / 'preprocess' / 'hp_terms.csv'\nMONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "MONDO_OBO_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "MONDO_OBO_FILE = str(project_config.PROJECT_DIR / 'preprocess' / 'mondo' / 'mondo.obo')\nHP_TERMS = project_config.PROJECT_DIR  / 'preprocess' / 'hp_terms.csv'\nMONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "HP_TERMS",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "HP_TERMS = project_config.PROJECT_DIR  / 'preprocess' / 'hp_terms.csv'\nMONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "MONDOTOHPO",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "MONDOTOHPO = project_config.PROJECT_DIR  /'preprocess'/ 'mondo' / 'mondo2hpo.csv'\n# output locations\nORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "ORPHANET_TO_MONDO_DICT",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "ORPHANET_TO_MONDO_DICT = str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl')\nHPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "HPO_TO_IDX_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "HPO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph' / project_config.CURR_KG / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl'\nHPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', ",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "HPO_TO_NAME_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "HPO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'hpo_to_name_dict_{project_config.CURR_KG}.pkl'\nENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', ",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "ENSEMBL_TO_IDX_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "ENSEMBL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'\nGENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', \n'MONDO:0008480':'MONDO:0031169', 'MONDO:0009641':'MONDO:0100294', 'MONDO:0010119':'MONDO:0031332',",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "GENE_SYMBOL_TO_IDX_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "GENE_SYMBOL_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'gene_symbol_to_idx_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', \n'MONDO:0008480':'MONDO:0031169', 'MONDO:0009641':'MONDO:0100294', 'MONDO:0010119':'MONDO:0031332',\n'MONDO:0010766':'MONDO:0100250', 'MONDO:0008646':'MONDO:0100316', 'MONDO:0018641':'MONDO:0100245', ",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "MONDO_TO_NAME_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "MONDO_TO_NAME_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG  / f'mondo_to_name_dict_{project_config.CURR_KG}.pkl'\nMONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', \n'MONDO:0008480':'MONDO:0031169', 'MONDO:0009641':'MONDO:0100294', 'MONDO:0010119':'MONDO:0031332',\n'MONDO:0010766':'MONDO:0100250', 'MONDO:0008646':'MONDO:0100316', 'MONDO:0018641':'MONDO:0100245', \n'MONDO:0010272':'MONDO:0010327', 'MONDO:0012189':'MONDO:0018274', 'MONDO:0007926':'MONDO:0100280', ",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "MONDO_TO_IDX_DICT_FILE",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "MONDO_TO_IDX_DICT_FILE = project_config.PROJECT_DIR / 'knowledge_graph'/ project_config.CURR_KG / f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl'\n# extracted from mondo.obo file\nOBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', \n'MONDO:0008480':'MONDO:0031169', 'MONDO:0009641':'MONDO:0100294', 'MONDO:0010119':'MONDO:0031332',\n'MONDO:0010766':'MONDO:0100250', 'MONDO:0008646':'MONDO:0100316', 'MONDO:0018641':'MONDO:0100245', \n'MONDO:0010272':'MONDO:0010327', 'MONDO:0012189':'MONDO:0018274', 'MONDO:0007926':'MONDO:0100280', \n'MONDO:0008032':'MONDO:0012215', 'MONDO:0009739':'MONDO:0024457', 'MONDO:0010419':'MONDO:0020721',",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "OBSOLETE_MONDO_DICT",
        "kind": 5,
        "importPath": "data_prep.preprocess_patients_and_kg",
        "description": "data_prep.preprocess_patients_and_kg",
        "peekOfCode": "OBSOLETE_MONDO_DICT = {'MONDO:0008646':'MONDO:0100316', 'MONDO:0016021': 'MONDO:0100062', \n'MONDO:0017125':'MONDO:0010261', 'MONDO:0010624':'MONDO:0100213', 'MONDO:0019863':'MONDO:0011812',\n'MONDO:0019523':'MONDO:0000171', 'MONDO:0018275':'MONDO:0018274', 'MONDO:0010071':'MONDO:0011939', 'MONDO:0010195':'MONDO:0008490', \n'MONDO:0008897':'MONDO:0100251', 'MONDO:0011127':'MONDO:0100344', 'MONDO:0009304':'MONDO:0012853', \n'MONDO:0008480':'MONDO:0031169', 'MONDO:0009641':'MONDO:0100294', 'MONDO:0010119':'MONDO:0031332',\n'MONDO:0010766':'MONDO:0100250', 'MONDO:0008646':'MONDO:0100316', 'MONDO:0018641':'MONDO:0100245', \n'MONDO:0010272':'MONDO:0010327', 'MONDO:0012189':'MONDO:0018274', 'MONDO:0007926':'MONDO:0100280', \n'MONDO:0008032':'MONDO:0012215', 'MONDO:0009739':'MONDO:0024457', 'MONDO:0010419':'MONDO:0020721',\n'MONDO:0007291':'MONDO:0031037', 'MONDO:0009245':'MONDO:0100339'} # to do starting with MONDO:0018275\ndef read_data(args):",
        "detail": "data_prep.preprocess_patients_and_kg",
        "documentation": {}
    },
    {
        "label": "tree",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "tree = ET.parse(str(project_config.PROJECT_DIR / 'data' / 'orphanet'/ 'raw'/ 'en_product1_5.8.2022.xml'))\nroot = tree.getroot()\ndisorderlist_cross = root[1]\nrel_list = []\nfor disorder in disorderlist_cross:\n    disorder_orphan = disorder.find(\"OrphaCode\").text\n    disorder_name = disorder.find(\"Name\").text\n    disorder_synonyms = \"|\".join([x.text\n                                  for x in disorder.find(\"SynonymList\")])\n    for child in disorder.find(\"ExternalReferenceList\"):",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "root = tree.getroot()\ndisorderlist_cross = root[1]\nrel_list = []\nfor disorder in disorderlist_cross:\n    disorder_orphan = disorder.find(\"OrphaCode\").text\n    disorder_name = disorder.find(\"Name\").text\n    disorder_synonyms = \"|\".join([x.text\n                                  for x in disorder.find(\"SynonymList\")])\n    for child in disorder.find(\"ExternalReferenceList\"):\n        external_source = child.find(\"Source\").text",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "disorderlist_cross",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "disorderlist_cross = root[1]\nrel_list = []\nfor disorder in disorderlist_cross:\n    disorder_orphan = disorder.find(\"OrphaCode\").text\n    disorder_name = disorder.find(\"Name\").text\n    disorder_synonyms = \"|\".join([x.text\n                                  for x in disorder.find(\"SynonymList\")])\n    for child in disorder.find(\"ExternalReferenceList\"):\n        external_source = child.find(\"Source\").text\n        external_id = child.find(\"Reference\").text",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "rel_list",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "rel_list = []\nfor disorder in disorderlist_cross:\n    disorder_orphan = disorder.find(\"OrphaCode\").text\n    disorder_name = disorder.find(\"Name\").text\n    disorder_synonyms = \"|\".join([x.text\n                                  for x in disorder.find(\"SynonymList\")])\n    for child in disorder.find(\"ExternalReferenceList\"):\n        external_source = child.find(\"Source\").text\n        external_id = child.find(\"Reference\").text\n        external_mapping_rel = child.find(\"DisorderMappingRelation\").find(\"Name\").text",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "rel_df",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "rel_df = pd.DataFrame(rel_list)\nprint(rel_df.columns)\norpha_to_omim_df = rel_df.loc[rel_df['External_Source'] == 'OMIM']\norpha_to_omim_df.to_csv(project_config.PROJECT_DIR / 'data' / 'orphanet'/ 'orphanet_to_omim_mapping_df.csv', index=False)\nprint(orpha_to_omim_df['External_Mapping_Status'].unique())\nprint(orpha_to_omim_df['External_Mapping_Rel'].unique())\norpha_to_omim_df_validated = orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Validated']\norpha_to_omim_df_validated_exact = orpha_to_omim_df_validated.loc[orpha_to_omim_df_validated['External_Mapping_Rel'] == 'E (Exact mapping: the two concepts are equivalent)']\nprint(len(orpha_to_omim_df), len(orpha_to_omim_df_validated), len(orpha_to_omim_df_validated_exact))\n#print(orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Not yet validated', ['OrphaNumber', 'Disorder_Name', 'External_ID', 'External_Mapping_Rel']])",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "orpha_to_omim_df",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "orpha_to_omim_df = rel_df.loc[rel_df['External_Source'] == 'OMIM']\norpha_to_omim_df.to_csv(project_config.PROJECT_DIR / 'data' / 'orphanet'/ 'orphanet_to_omim_mapping_df.csv', index=False)\nprint(orpha_to_omim_df['External_Mapping_Status'].unique())\nprint(orpha_to_omim_df['External_Mapping_Rel'].unique())\norpha_to_omim_df_validated = orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Validated']\norpha_to_omim_df_validated_exact = orpha_to_omim_df_validated.loc[orpha_to_omim_df_validated['External_Mapping_Rel'] == 'E (Exact mapping: the two concepts are equivalent)']\nprint(len(orpha_to_omim_df), len(orpha_to_omim_df_validated), len(orpha_to_omim_df_validated_exact))\n#print(orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Not yet validated', ['OrphaNumber', 'Disorder_Name', 'External_ID', 'External_Mapping_Rel']])\n# print(orpha_to_omim_df.loc[['OrphaNumber', 'Disorder_Name', 'External_Source', 'External_ID',\n#        'External_Mapping_Rel', 'External_Mapping_Status']])",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "orpha_to_omim_df_validated",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "orpha_to_omim_df_validated = orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Validated']\norpha_to_omim_df_validated_exact = orpha_to_omim_df_validated.loc[orpha_to_omim_df_validated['External_Mapping_Rel'] == 'E (Exact mapping: the two concepts are equivalent)']\nprint(len(orpha_to_omim_df), len(orpha_to_omim_df_validated), len(orpha_to_omim_df_validated_exact))\n#print(orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Not yet validated', ['OrphaNumber', 'Disorder_Name', 'External_ID', 'External_Mapping_Rel']])\n# print(orpha_to_omim_df.loc[['OrphaNumber', 'Disorder_Name', 'External_Source', 'External_ID',\n#        'External_Mapping_Rel', 'External_Mapping_Status']])",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "orpha_to_omim_df_validated_exact",
        "kind": 5,
        "importPath": "data_prep.process_orpha_to_omim_map",
        "description": "data_prep.process_orpha_to_omim_map",
        "peekOfCode": "orpha_to_omim_df_validated_exact = orpha_to_omim_df_validated.loc[orpha_to_omim_df_validated['External_Mapping_Rel'] == 'E (Exact mapping: the two concepts are equivalent)']\nprint(len(orpha_to_omim_df), len(orpha_to_omim_df_validated), len(orpha_to_omim_df_validated_exact))\n#print(orpha_to_omim_df.loc[orpha_to_omim_df['External_Mapping_Status'] == 'Not yet validated', ['OrphaNumber', 'Disorder_Name', 'External_ID', 'External_Mapping_Rel']])\n# print(orpha_to_omim_df.loc[['OrphaNumber', 'Disorder_Name', 'External_Source', 'External_ID',\n#        'External_Mapping_Rel', 'External_Mapping_Status']])",
        "detail": "data_prep.process_orpha_to_omim_map",
        "documentation": {}
    },
    {
        "label": "GPAligner",
        "kind": 6,
        "importPath": "shepherd.task_heads.gp_aligner",
        "description": "shepherd.task_heads.gp_aligner",
        "peekOfCode": "class GPAligner(pl.LightningModule):\n    def __init__(self, hparams, embed_dim):\n        super().__init__()\n        self.hyperparameters = hparams\n        print('GPAligner embedding dimension: ', embed_dim)\n        # attention for collapsing set of phenotype embeddings\n        self.attn_vector = nn.Parameter(torch.zeros((1, embed_dim), dtype=torch.float), requires_grad=True)   \n        nn.init.xavier_uniform_(self.attn_vector)\n        if self.hyperparameters['attention_type'] == 'bilinear':\n            self.attention = BilinearAttention(embed_dim, embed_dim)",
        "detail": "shepherd.task_heads.gp_aligner",
        "documentation": {}
    },
    {
        "label": "PatientNCA",
        "kind": 6,
        "importPath": "shepherd.task_heads.patient_nca",
        "description": "shepherd.task_heads.patient_nca",
        "peekOfCode": "class PatientNCA(pl.LightningModule):\n    def __init__(self, hparams, embed_dim):\n        super().__init__()\n        self.hyperparameters = hparams\n        # attention for collapsing set of phenotype embeddings\n        self.attn_vector = nn.Parameter(torch.zeros((1, embed_dim), dtype=torch.float), requires_grad=True)   \n        nn.init.xavier_uniform_(self.attn_vector)\n        if self.hyperparameters['attention_type'] == 'bilinear':\n            self.attention = BilinearAttention(embed_dim, embed_dim)\n        elif self.hyperparameters['attention_type'] == 'additive':",
        "detail": "shepherd.task_heads.patient_nca",
        "documentation": {}
    },
    {
        "label": "MultisimilarityCriterion",
        "kind": 6,
        "importPath": "shepherd.utils.loss_utils",
        "description": "shepherd.utils.loss_utils",
        "peekOfCode": "class MultisimilarityCriterion(torch.nn.Module):\n    def __init__(self, pos_weight, neg_weight, margin, thresh, \n                 embed_dim, only_hard_distractors=True):\n        super().__init__()\n        self.pos_weight = pos_weight\n        self.neg_weight = neg_weight\n        self.margin     = margin\n        self.thresh     = thresh\n        self.only_hard_distractors = only_hard_distractors\n    def forward(self, sims, mask, one_hot_labels, **kwargs):",
        "detail": "shepherd.utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "NCALoss",
        "kind": 6,
        "importPath": "shepherd.utils.loss_utils",
        "description": "shepherd.utils.loss_utils",
        "peekOfCode": "class NCALoss(BaseMetricLossFunction):\n    def __init__(self, softmax_scale=1, only_hard_distractors=False, **kwargs):\n        super().__init__(**kwargs)\n        self.softmax_scale = softmax_scale\n        self.only_hard_distractors = only_hard_distractors\n        self.add_to_recordable_attributes(\n            list_of_names=[\"softmax_scale\"], is_stat=False\n        )\n    def forward(self, phenotype_embedding, disease_embedding, batch_disease_nid, batch_cand_disease_nid=None, disease_mask=None, one_hot_labels=None, indices_tuple=None, use_candidate_list=False):\n        \"\"\"",
        "detail": "shepherd.utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "unique",
        "kind": 2,
        "importPath": "shepherd.utils.loss_utils",
        "description": "shepherd.utils.loss_utils",
        "peekOfCode": "def unique(x, dim=None):\n        \"\"\"Unique elements of x and indices of those unique elements\n        https://github.com/pytorch/pytorch/issues/36748#issuecomment-619514810\n        e.g.\n        unique(tensor([\n            [1, 2, 3],\n            [1, 2, 4],\n            [1, 2, 3],\n            [1, 2, 5]\n        ]), dim=0)",
        "detail": "shepherd.utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "construct_batch_labels",
        "kind": 2,
        "importPath": "shepherd.utils.loss_utils",
        "description": "shepherd.utils.loss_utils",
        "peekOfCode": "def construct_batch_labels(candidate_embeddings, candidate_node_idx, correct_node_idx, mask):\n    '''\n        Format the batch to input into metric learning loss function\n    '''\n    batch, n_candidates, embed_dim = candidate_embeddings.shape\n    # get mask\n    mask_reshaped = mask.reshape(batch*n_candidates, -1)\n    expanded_mask = mask_reshaped.expand(-1,embed_dim) \n    # flatten the gene node idx and gene embeddings\n    candidate_node_idx_flattened = candidate_node_idx.view(batch*n_candidates, -1)",
        "detail": "shepherd.utils.loss_utils",
        "documentation": {}
    },
    {
        "label": "HeterogeneousEdgeIndex",
        "kind": 6,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "class HeterogeneousEdgeIndex(NamedTuple): #adopted from NeighborSampler code in Pytorch Geometric\n    edge_index: Tensor\n    e_id: Optional[Tensor]\n    edge_type: Optional[Tensor]\n    size: Tuple[int, int]\n    def to(self, *args, **kwargs):\n        edge_index = self.edge_index.to(*args, **kwargs)\n        e_id = self.e_id.to(*args, **kwargs) if self.e_id is not None else None\n        edge_type = self.edge_type.to(*args, **kwargs) if self.edge_type is not None else None\n        return EdgeIndex(edge_index, e_id, edge_type, self.size)",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def to_numpy(input):\n        if isinstance(input, torch.sparse.FloatTensor):\n            return input.to_dense().cpu().detach().numpy()\n        else:\n            return input.cpu().detach().numpy()\ndef from_numpy(np_array):\n    return torch.as_tensor(np_array)\ndef sample_node_for_et(et, targets):\n    neg_idx = torch.randperm(targets[et].shape[0])[0] # Randomly select an index into the targets for a given edge type\n    node = targets[et][neg_idx] # Select the location of that edge type",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "from_numpy",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def from_numpy(np_array):\n    return torch.as_tensor(np_array)\ndef sample_node_for_et(et, targets):\n    neg_idx = torch.randperm(targets[et].shape[0])[0] # Randomly select an index into the targets for a given edge type\n    node = targets[et][neg_idx] # Select the location of that edge type\n    return node\nclass HeterogeneousEdgeIndex(NamedTuple): #adopted from NeighborSampler code in Pytorch Geometric\n    edge_index: Tensor\n    e_id: Optional[Tensor]\n    edge_type: Optional[Tensor]",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "sample_node_for_et",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def sample_node_for_et(et, targets):\n    neg_idx = torch.randperm(targets[et].shape[0])[0] # Randomly select an index into the targets for a given edge type\n    node = targets[et][neg_idx] # Select the location of that edge type\n    return node\nclass HeterogeneousEdgeIndex(NamedTuple): #adopted from NeighborSampler code in Pytorch Geometric\n    edge_index: Tensor\n    e_id: Optional[Tensor]\n    edge_type: Optional[Tensor]\n    size: Tuple[int, int]\n    def to(self, *args, **kwargs):",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_batched_data",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def get_batched_data(data, all_data):\n    batch_size, n_id, adjs = data\n    adjs = [HeterogeneousEdgeIndex(adj.edge_index, adj.e_id, all_data.edge_attr[adj.e_id], adj.size) for adj in adjs] \n    data = Data(adjs = adjs, \n                batch_size = batch_size,\n                n_id = n_id, \n                )\n    return data\nMAX_SIZE = 625\ndef get_mask(edge_index, nodes, ind):",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_mask",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def get_mask(edge_index, nodes, ind):\n    n_splits = math.ceil(nodes.size(0) / MAX_SIZE)\n    node_mask = (edge_index[ind,:] == nodes[:MAX_SIZE].unsqueeze(-1)).nonzero()\n    for i in range(1, n_splits-1):\n        node_mask_mid = (edge_index[ind,:] == nodes[MAX_SIZE*i:MAX_SIZE*(i+1)].unsqueeze(-1)).nonzero()\n        node_mask_mid[:,0] = node_mask_mid[:,0] + (MAX_SIZE*i)\n        node_mask = torch.cat([node_mask, node_mask_mid])\n    node_mask_end = (edge_index[ind,:] == nodes[MAX_SIZE*(n_splits-1):].unsqueeze(-1)).nonzero()\n    node_mask_end[:,0] = node_mask_end[:,0] + (MAX_SIZE*(n_splits-1))\n    node_mask = torch.cat([node_mask, node_mask_end])",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_indices_into_edge_index",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def get_indices_into_edge_index(edge_index, source_nodes, target_nodes):\n    if source_nodes.size(0) > MAX_SIZE:\n        source_node_mask = get_mask(edge_index, source_nodes, ind = 0)\n        target_node_mask = get_mask(edge_index, target_nodes, ind = 1)\n    else:\n        source_node_mask = (edge_index[0,:] == source_nodes.unsqueeze(-1)).nonzero()\n        target_node_mask = (edge_index[1,:] == target_nodes.unsqueeze(-1)).nonzero()\n    vals_pos, counts_pos = torch.unique(torch.cat([source_node_mask, target_node_mask]), return_counts=True, dim=0)\n    return vals_pos[counts_pos > 1][:,1], vals_pos[counts_pos > 1][:,0]\ndef get_edges(data, all_data, dataset_type):",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "get_edges",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def get_edges(data, all_data, dataset_type):\n    # get edge index\n    edge_index = all_data.edge_index[:, all_data[f'{dataset_type}_mask']].to(data.n_id.device)\n    edge_type = all_data.edge_attr[ all_data[f'{dataset_type}_mask']].to(data.n_id.device)\n    # filter to edges between \"seed nodes\"\n    source_nodes = data.n_id[:int(data.batch_size/2)]\n    pos_target_nodes = data.n_id[int(data.batch_size/2):int(data.batch_size)]\n    # get index into edge & node list\n    ind_to_edge_index_pos, ind_to_nodes_pos = get_indices_into_edge_index(edge_index, source_nodes, pos_target_nodes)\n    # get edges where both source & target are seed nodes",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "calc_metrics",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def calc_metrics(pred, y, threshold=0.5):\n    y[y < 0] = 0\n    try: \n        roc_score = roc_auc_score(y, pred)\n    except ValueError: \n        roc_score = 0.5 \n    ap_score = average_precision_score(y, pred)\n    acc = accuracy_score(y, pred > threshold)\n    f1 = f1_score(y, pred > threshold, average = 'micro')\n    return roc_score, ap_score, acc, f1",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "metrics_per_rel",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def metrics_per_rel(pred, link_labels, edge_attr_dict, total_edge_type, split, threshold=0.5, verbose=False):\n    log = {}\n    for attr, idx in edge_attr_dict.items():\n        mask = (total_edge_type == idx)\n        if mask.sum() == 0: continue\n        pred_per_rel = pred[mask]\n        y_per_rel = link_labels[mask]\n        roc_per_rel, ap_per_rel, acc_per_rel, f1_per_rel = calc_metrics(pred_per_rel.cpu().detach().numpy(), y_per_rel.cpu().detach().numpy(), threshold)\n        if verbose:\n            print(\"ROC for edge type {}: {:.5f}\".format(attr, roc_per_rel))",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "plot_roc_curve",
        "kind": 2,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "def plot_roc_curve(pred, labels):\n    fpr, tpr, thresholds = roc_curve(labels, pred)\n    gmeans = np.sqrt(tpr * (1-fpr))\n    max_gmean = max(gmeans)\n    roc = roc_auc_score(labels, pred)\n    data = {\"False Positive Rate\": fpr, \"True Positive Rate\": tpr, \"Threshold\": thresholds, \n            \"ROC\": [roc] * len(thresholds), \"G-Mean\": gmeans, \"Max G-Mean\": [max_gmean] * len(thresholds)}\n    df = pd.DataFrame(data)\n    fig = px.line(df, x = \"False Positive Rate\", y = \"True Positive Rate\", hover_data=list(data.keys()))\n    return fig",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef to_numpy(input):\n        if isinstance(input, torch.sparse.FloatTensor):\n            return input.to_dense().cpu().detach().numpy()\n        else:\n            return input.cpu().detach().numpy()\ndef from_numpy(np_array):\n    return torch.as_tensor(np_array)\ndef sample_node_for_et(et, targets):\n    neg_idx = torch.randperm(targets[et].shape[0])[0] # Randomly select an index into the targets for a given edge type",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "MAX_SIZE",
        "kind": 5,
        "importPath": "shepherd.utils.pretrain_utils",
        "description": "shepherd.utils.pretrain_utils",
        "peekOfCode": "MAX_SIZE = 625\ndef get_mask(edge_index, nodes, ind):\n    n_splits = math.ceil(nodes.size(0) / MAX_SIZE)\n    node_mask = (edge_index[ind,:] == nodes[:MAX_SIZE].unsqueeze(-1)).nonzero()\n    for i in range(1, n_splits-1):\n        node_mask_mid = (edge_index[ind,:] == nodes[MAX_SIZE*i:MAX_SIZE*(i+1)].unsqueeze(-1)).nonzero()\n        node_mask_mid[:,0] = node_mask_mid[:,0] + (MAX_SIZE*i)\n        node_mask = torch.cat([node_mask, node_mask_mid])\n    node_mask_end = (edge_index[ind,:] == nodes[MAX_SIZE*(n_splits-1):].unsqueeze(-1)).nonzero()\n    node_mask_end[:,0] = node_mask_end[:,0] + (MAX_SIZE*(n_splits-1))",
        "detail": "shepherd.utils.pretrain_utils",
        "documentation": {}
    },
    {
        "label": "mean_reciprocal_rank",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def mean_reciprocal_rank(correct_gene_ranks):\n    return torch.mean(1/correct_gene_ranks)\ndef average_rank(correct_gene_ranks):\n    return torch.mean(correct_gene_ranks)\ndef top_k_acc(correct_gene_ranks, k):\n    return torch.sum(correct_gene_ranks <= k) / len(correct_gene_ranks)\n###########################################\n# below functions from AllenNLP\ndef masked_mean(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "average_rank",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def average_rank(correct_gene_ranks):\n    return torch.mean(correct_gene_ranks)\ndef top_k_acc(correct_gene_ranks, k):\n    return torch.sum(correct_gene_ranks <= k) / len(correct_gene_ranks)\n###########################################\n# below functions from AllenNLP\ndef masked_mean(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False\n    ) -> torch.Tensor:\n    \"\"\"",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "top_k_acc",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def top_k_acc(correct_gene_ranks, k):\n    return torch.sum(correct_gene_ranks <= k) / len(correct_gene_ranks)\n###########################################\n# below functions from AllenNLP\ndef masked_mean(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False\n    ) -> torch.Tensor:\n    \"\"\"\n    To calculate mean along certain dimensions on masked values\n    # Parameters",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "masked_mean",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def masked_mean(\n    vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool = False\n    ) -> torch.Tensor:\n    \"\"\"\n    To calculate mean along certain dimensions on masked values\n    # Parameters\n    vector : `torch.Tensor`\n        The vector to calculate mean.\n    mask : `torch.BoolTensor`\n        The mask of the vector. It must be broadcastable with vector.",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "tiny_value_of_dtype",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def tiny_value_of_dtype(dtype: torch.dtype):\n    \"\"\"\n    Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical\n    issues such as division by zero.\n    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.\n    Only supports floating point dtypes.\n    \"\"\"\n    if not dtype.is_floating_point:\n        raise TypeError(\"Only supports floating point dtypes.\")\n    if dtype == torch.float or dtype == torch.double:",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "masked_softmax",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def masked_softmax(\n    vector: torch.Tensor,\n    mask: torch.BoolTensor,\n    dim: int = -1,\n    memory_efficient: bool = False,\n    ) -> torch.Tensor:\n    \"\"\"\n    `torch.nn.functional.softmax(vector)` does not work if some elements of `vector` should be\n    masked.  This performs a softmax on just the non-masked portions of `vector`.  Passing\n    `None` in for the mask is also acceptable; you'll just get a regular softmax.",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "weighted_sum",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def weighted_sum(matrix: torch.Tensor, attention: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an\n    \"attention\" vector), and returns a weighted sum of the rows in the matrix.  This is the typical\n    computation performed after an attention mechanism.\n    Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle\n    higher-order tensors.  We always sum over the second-to-last dimension of the \"matrix\", and we\n    assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the\n    \"vector\".  Non-matched dimensions in the \"vector\" must be `directly after the batch dimension`.\n    For example, say I have a \"matrix\" with dimensions `(batch_size, num_queries, num_words,",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "mrr_vs_percent_overlap",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def mrr_vs_percent_overlap(correct_gene_rank, percent_overlap_train):\n    df = pd.DataFrame({\"Percent of Phenotypes Found in Single Train Patient\": percent_overlap_train.squeeze(), \"Rank of Correct Gene\": correct_gene_rank})\n    fig = px.scatter(df, x = \"Percent of Phenotypes Found in Single Train Patient\", y = \"Rank of Correct Gene\")\n    return fig\ndef plot_softmax(softmax):\n    softmax = [s.detach().item() for s in softmax]\n    df = pd.DataFrame({'softmax':softmax})\n    fig = px.histogram(df, x=\"softmax\")\n    return fig\ndef fit_umap(embed, labels={}, n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', random_state=3):",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_softmax",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_softmax(softmax):\n    softmax = [s.detach().item() for s in softmax]\n    df = pd.DataFrame({'softmax':softmax})\n    fig = px.histogram(df, x=\"softmax\")\n    return fig\ndef fit_umap(embed, labels={}, n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', random_state=3):\n    embed = embed.detach().cpu()\n    mapping = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=metric, random_state=random_state).fit(embed)\n    embedding = mapping.transform(embed)\n    data = {\"x\": embedding[:, 0], \"y\": embedding[:, 1]}",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "fit_umap",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def fit_umap(embed, labels={}, n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', random_state=3):\n    embed = embed.detach().cpu()\n    mapping = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=metric, random_state=random_state).fit(embed)\n    embedding = mapping.transform(embed)\n    data = {\"x\": embedding[:, 0], \"y\": embedding[:, 1]}\n    if len(labels) > 0: data.update(labels)\n    df = pd.DataFrame(data)\n    if len(labels) > 0: fig = px.scatter(df, x = \"x\", y = \"y\", color = \"Node Type\", hover_data=list(labels.keys()))\n    else: fig = px.scatter(df, x = \"x\", y = \"y\")\n    return fig",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_degree_vs_attention",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_degree_vs_attention(attn_weights, phenotype_names, single_patient=False):\n    if single_patient:\n        phenotype_names = phenotype_names[0]\n        attn_weights = attn_weights[0]\n        data = [(w.item(), p_name[1]) for w, p_name in zip(attn_weights, phenotype_names)]\n    else:\n        data = [(attn_weights[i], phenotype_names[i]) for i in range(len(phenotype_names))]\n        data = [(w.item(), p_name[1]) for attn_w, phen_name in data for w, p_name in zip(attn_w, phen_name)] \n    attn_weights, degrees = zip(*data)\n    df = pd.DataFrame({\"Node Degree\": degrees, \"Attention Weight\": attn_weights})",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_nhops_to_gene_vs_attention",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_nhops_to_gene_vs_attention(attn_weights, phenotype_names, nhops_g_p, single_patient=False):\n    if single_patient:\n        attn_weights = attn_weights[0]\n        nhops_g_p = nhops_g_p[0]\n        phenotype_names = phenotype_names[0]\n        data = [(w.item(), hops) for w, hops in zip(attn_weights, nhops_g_p)]\n    else:\n        data = [(attn_weights[i], nhops_g_p[i]) for i in range(len(phenotype_names))]\n        data = [(w.item(), hop) for attn_w, nhops in data for w, hop in zip(attn_w, nhops)] \n    attn_weights, n_hops_g_p = zip(*data)",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_x_intrain",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_gene_rank_vs_x_intrain(corr_gene_ranks, in_train):\n    if sum(in_train == 1) == 0: \n        values_in_train = -1\n        err_in_train = 0\n    else: \n        values_in_train = torch.mean(corr_gene_ranks[in_train == 1].float())\n        err_in_train = torch.std(corr_gene_ranks[in_train == 1].float())\n    if sum(in_train == 0) == 0: \n        values_not_in_train = -1\n        err_not_in_train = 0",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_numtrain",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_gene_rank_vs_numtrain(corr_gene_ranks, correct_gene_nid, train_corr_gene_nid):\n    gene_counts = [train_corr_gene_nid[g] if g in train_corr_gene_nid else 0 for g in list(correct_gene_nid.numpy())]\n    data = {\"Rank of Correct Gene\": corr_gene_ranks.cpu(), \"Number of Times Seen\": gene_counts, \"Gene ID\": correct_gene_nid}\n    df = pd.DataFrame(data)\n    fig = px.scatter(df, x = \"Number of Times Seen\", y = \"Rank of Correct Gene\", hover_data=list(data.keys()))\n    return fig, gene_counts\ndef plot_gene_rank_vs_trainset(corr_gene_ranks, correct_gene_nid, gene_counts): # train_corr_gene_nid has dimension num_gene x num_sets (corr, cand, sparse, target)\n    trainset_labels = [\"-\".join([str(int(l)) for l in list((gene_counts[i, :] > 0).numpy())]) for i in range(gene_counts.shape[0])]\n    gene_ranks_dict = {}\n    for l, r in zip(trainset_labels, corr_gene_ranks): # (corr, cand, sparse, target)",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_trainset",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_gene_rank_vs_trainset(corr_gene_ranks, correct_gene_nid, gene_counts): # train_corr_gene_nid has dimension num_gene x num_sets (corr, cand, sparse, target)\n    trainset_labels = [\"-\".join([str(int(l)) for l in list((gene_counts[i, :] > 0).numpy())]) for i in range(gene_counts.shape[0])]\n    gene_ranks_dict = {}\n    for l, r in zip(trainset_labels, corr_gene_ranks): # (corr, cand, sparse, target)\n        l_full = []\n        if l.split(\"-\")[0] == \"1\": l_full.append(\"Corr\")\n        if l.split(\"-\")[1] == \"1\": l_full.append(\"Cand\")\n        if l.split(\"-\")[2] == \"1\": l_full.append(\"Sparse\")\n        if l.split(\"-\")[3] == \"1\": l_full.append(\"Target\")\n        if len(l_full) == 0: l_full = \"None\"",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_fraction_phenotype",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_gene_rank_vs_fraction_phenotype(corr_gene_ranks, frac_p):\n    df = pd.DataFrame({\"Rank of Correct Gene\": corr_gene_ranks, \"Fraction of Phenotypes\": frac_p})\n    df = df[df[\"Fraction of Phenotypes\"] > -1]\n    fig = px.scatter(df, x = \"Fraction of Phenotypes\", y = \"Rank of Correct Gene\")\n    return fig\ndef plot_gene_rank_vs_hops(corr_gene_ranks, n_hops):\n    mean_hops = []\n    min_hops = []\n    for hops in n_hops:\n        if type(hops) == list: # gene to phenotype n_hops",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "plot_gene_rank_vs_hops",
        "kind": 2,
        "importPath": "shepherd.utils.train_utils",
        "description": "shepherd.utils.train_utils",
        "peekOfCode": "def plot_gene_rank_vs_hops(corr_gene_ranks, n_hops):\n    mean_hops = []\n    min_hops = []\n    for hops in n_hops:\n        if type(hops) == list: # gene to phenotype n_hops\n            mean_hops.append(np.mean(hops))\n            min_hops.append(np.min(hops))\n        else: # phenotype to phenotype n_hops\n            filtered_hops = torch.cat([hops[i][hops[i] > 0] for i in range(hops.shape[0])]).float()\n            mean_hops.append(torch.mean(filtered_hops).item())",
        "detail": "shepherd.utils.train_utils",
        "documentation": {}
    },
    {
        "label": "PatientDataset",
        "kind": 6,
        "importPath": "shepherd.dataset",
        "description": "shepherd.dataset",
        "peekOfCode": "class PatientDataset(Dataset):\n    def __init__(self, filepath, gp_spl=None, raw_data=False, mondo_map_file=str(project_config.PROJECT_DIR / 'mondo_references.csv'), needs_disease_mapping=False, time=False): \n        self.filepath = filepath\n        self.patients = read_patients(filepath)\n        print('Dataset filepath: ', filepath)\n        print('Number of patients: ', len(self.patients))\n        # add placeholder for true genes/diseases if they don't exist\n        for patient in self.patients:\n            if 'true_genes' not in patient: patient['true_genes'] = []\n            if 'true_diseases' not in patient: patient['true_diseases'] = []",
        "detail": "shepherd.dataset",
        "documentation": {}
    },
    {
        "label": "bilinear",
        "kind": 2,
        "importPath": "shepherd.decoders",
        "description": "shepherd.decoders",
        "peekOfCode": "def bilinear(s, r, t):\n    return torch.sum(s * r * t, dim = 1)\ndef trans(s, r, t):\n    return -torch.norm(s + r - t, dim = 1)\ndef dot(s, t):\n    return torch.sum(s * t, dim = 1)",
        "detail": "shepherd.decoders",
        "documentation": {}
    },
    {
        "label": "trans",
        "kind": 2,
        "importPath": "shepherd.decoders",
        "description": "shepherd.decoders",
        "peekOfCode": "def trans(s, r, t):\n    return -torch.norm(s + r - t, dim = 1)\ndef dot(s, t):\n    return torch.sum(s * t, dim = 1)",
        "detail": "shepherd.decoders",
        "documentation": {}
    },
    {
        "label": "dot",
        "kind": 2,
        "importPath": "shepherd.decoders",
        "description": "shepherd.decoders",
        "peekOfCode": "def dot(s, t):\n    return torch.sum(s * t, dim = 1)",
        "detail": "shepherd.decoders",
        "documentation": {}
    },
    {
        "label": "CombinedGPAligner",
        "kind": 6,
        "importPath": "shepherd.gene_prioritization_model",
        "description": "shepherd.gene_prioritization_model",
        "peekOfCode": "class CombinedGPAligner(pl.LightningModule):\n    def __init__(self, edge_attr_dict, all_data, n_nodes=None, node_ckpt=None, hparams=None, node_hparams=None,  spl_pca=[], spl_gate=[]):\n        super().__init__()\n        print('Initializing Model')\n        self.save_hyperparameters('hparams', ignore=[\"spl_pca\", \"spl_gate\"]) # spl_pca and spl_gate never get used\n        #print('Saved combined model hyperparameters: ', self.hparams)\n        self.all_data = all_data\n        self.all_train_nodes = {}\n        self.train_patient_nodes = {}\n        self.train_sparse_nodes = {}",
        "detail": "shepherd.gene_prioritization_model",
        "documentation": {}
    },
    {
        "label": "get_pretrain_hparams",
        "kind": 2,
        "importPath": "shepherd.hparams",
        "description": "shepherd.hparams",
        "peekOfCode": "def get_pretrain_hparams(args, combined=False):    \n    print('node embedder args: ', args)\n    # Default\n    hparams = {\n               # Tunable parameters\n               'nfeat': args.nfeat if not combined else 4096,\n               'hidden': args.hidden if not combined else 256,\n               'output': args.output if not combined else 128,\n               'n_heads': args.n_heads if not combined else 2,\n               'wd': args.wd if not combined else 5e-4,",
        "detail": "shepherd.hparams",
        "documentation": {}
    },
    {
        "label": "get_train_hparams",
        "kind": 2,
        "importPath": "shepherd.hparams",
        "description": "shepherd.hparams",
        "peekOfCode": "def get_train_hparams(args):\n    print('Train model args: ', args)\n    # Default\n    hparams = {\n               # Tunable parameters\n               'sparse_sample': args.sparse_sample, # Randomly sample N nodes from KG\n               'lr': args.lr,\n               'upsample_cand': args.upsample_cand, \n               'neighbor_sampler_sizes': [args.neighbor_sampler_size, 10, 5],\n               'lambda': args.lmbda, # Contribution of two loss functions",
        "detail": "shepherd.hparams",
        "documentation": {}
    },
    {
        "label": "get_run_type_args",
        "kind": 2,
        "importPath": "shepherd.hparams",
        "description": "shepherd.hparams",
        "peekOfCode": "def get_run_type_args(args, hparams):\n    if args.run_type == 'causal_gene_discovery':\n        hparams.update({\n                        'model_type': 'aligner', \n                        'loss': 'gene_multisimilarity', \n                        'use_diseases': False,\n                        'add_cand_diseases': False,\n                        'add_similar_patients': False,\n                        'wandb_project_name': 'causal-gene-discovery'\n                       })",
        "detail": "shepherd.hparams",
        "documentation": {}
    },
    {
        "label": "get_patient_data_args",
        "kind": 2,
        "importPath": "shepherd.hparams",
        "description": "shepherd.hparams",
        "peekOfCode": "def get_patient_data_args(args, hparams):\n    if args.patient_data == \"disease_simulated\":\n        hparams.update({'train_data': f'simulated_patients/disease_split_train_sim_patients_{project_config.CURR_KG}.txt',\n                        'validation_data': f'simulated_patients/disease_split_val_sim_patients_{project_config.CURR_KG}.txt', \n                        'test_data': f'simulated_patients/disease_split_val_sim_patients_{project_config.CURR_KG}.txt',\n                        'spl': f'simulated_patients/disease_split_all_sim_patients_{project_config.CURR_KG}_spl_matrix.npy',\n                        'spl_index': f'simulated_patients/disease_split_all_sim_patients_{project_config.CURR_KG}_spl_index_dict.pkl'\n                        })\n    elif args.patient_data == \"test_predict\":\n        hparams.update({",
        "detail": "shepherd.hparams",
        "documentation": {}
    },
    {
        "label": "get_predict_hparams",
        "kind": 2,
        "importPath": "shepherd.hparams",
        "description": "shepherd.hparams",
        "peekOfCode": "def get_predict_hparams(args):\n    hparams = {\n               'seed': 33,\n               'n_gpus': 0, # NOTE: currently predict scripts only work with CPU\n               'num_workers': 4, \n               'profiler': 'simple',\n               'pin_memory': False,\n               'time': False,\n               'log_gpu_memory': False,\n               'debug': False,",
        "detail": "shepherd.hparams",
        "documentation": {}
    },
    {
        "label": "NodeEmbeder",
        "kind": 6,
        "importPath": "shepherd.node_embedder_model",
        "description": "shepherd.node_embedder_model",
        "peekOfCode": "class NodeEmbeder(pl.LightningModule):\n    def __init__(self, all_data, edge_attr_dict, hp_dict=None, num_nodes=None, combined_training=False, spl_mat=[]):\n        super().__init__()\n        # save hyperparameters\n        self.save_hyperparameters(\"hp_dict\", ignore=[\"spl_mat\"])\n        # Data\n        self.all_data = all_data\n        self.edge_attr_dict = edge_attr_dict\n        # Model parameters\n        self.lr = self.hparams.hp_dict['lr']",
        "detail": "shepherd.node_embedder_model",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "shepherd.node_embedder_model",
        "description": "shepherd.node_embedder_model",
        "peekOfCode": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass NodeEmbeder(pl.LightningModule):\n    def __init__(self, all_data, edge_attr_dict, hp_dict=None, num_nodes=None, combined_training=False, spl_mat=[]):\n        super().__init__()\n        # save hyperparameters\n        self.save_hyperparameters(\"hp_dict\", ignore=[\"spl_mat\"])\n        # Data\n        self.all_data = all_data\n        self.edge_attr_dict = edge_attr_dict\n        # Model parameters",
        "detail": "shepherd.node_embedder_model",
        "documentation": {}
    },
    {
        "label": "CombinedPatientNCA",
        "kind": 6,
        "importPath": "shepherd.patient_nca_model",
        "description": "shepherd.patient_nca_model",
        "peekOfCode": "class CombinedPatientNCA(pl.LightningModule):\n    def __init__(self, edge_attr_dict, all_data, n_nodes=None, node_ckpt=None, hparams=None):\n        super().__init__()\n        self.save_hyperparameters('hparams') \n        #print('Saved combined model hyperparameters: ', self.hparams)\n        self.all_data = all_data\n        self.all_train_nodes = []\n        self.train_patient_nodes = []\n        print(f\"Loading Node Embedder from {node_ckpt}\")\n        # NOTE: loads in saved hyperparameters",
        "detail": "shepherd.patient_nca_model",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "shepherd.predict",
        "description": "shepherd.predict",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Predict using SHEPHERD\")\n    parser.add_argument(\"--edgelist\", type=str, default=None, help=\"File with edge list\")\n    parser.add_argument(\"--node_map\", type=str, default=None, help=\"File with node list\")\n    parser.add_argument('--patient_data', default=\"disease_simulated\", type=str)\n    parser.add_argument('--run_type', choices=[\"causal_gene_discovery\", \"disease_characterization\", \"patients_like_me\"], type=str)\n    parser.add_argument('--saved_node_embeddings_path', type=str, default=None, help='Path to pretrained model checkpoint')\n    parser.add_argument('--best_ckpt', type=str, default=None, help='Name of the best performing checkpoint')\n    args = parser.parse_args()\n    return args",
        "detail": "shepherd.predict",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "shepherd.predict",
        "description": "shepherd.predict",
        "peekOfCode": "def predict(args):\n    # Hyperparameters\n    hparams = get_predict_hparams(args)\n    # Seed\n    pl.seed_everything(hparams['seed'])\n    # Read KG\n    all_data, edge_attr_dict, nodes = preprocess.preprocess_graph(args)\n    n_nodes = len(nodes[\"node_idx\"].unique())\n    gene_phen_dis_node_idx = torch.LongTensor(nodes.loc[nodes['node_type'].isin(['gene/protein', 'effect/phenotype', 'disease']), 'node_idx'].values)\n    # Get dataset",
        "detail": "shepherd.predict",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_LAUNCH_BLOCKING']",
        "kind": 5,
        "importPath": "shepherd.predict",
        "description": "shepherd.predict",
        "peekOfCode": "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n'''\nExample Command:\npython predict.py \\\n--run_type causal_gene_discovery \\\n--patient_data test_predict \\\n--edgelist KG_edgelist_mask.txt \\\n--node_map KG_node_map.txt \\\n--saved_node_embeddings_path checkpoints/pretrain.ckpt \\\n--best_ckpt checkpoints/causal_gene_discovery.ckpt ",
        "detail": "shepherd.predict",
        "documentation": {}
    },
    {
        "label": "preprocess_graph",
        "kind": 2,
        "importPath": "shepherd.preprocess",
        "description": "shepherd.preprocess",
        "peekOfCode": "def preprocess_graph(args):\n    # Read in nodes & edges\n    nodes = pd.read_csv(project_config.KG_DIR / args.node_map, sep=\"\\t\")\n    edges = pd.read_csv(project_config.KG_DIR / args.edgelist, sep=\"\\t\")\n    # Initialize edge index\n    edge_index = torch.LongTensor(edges[['x_idx', 'y_idx']].values.T).contiguous() \n    edge_attr = edges['full_relation']\n    # Convert edge attributes to idx\n    edge_attr_list = [\n                      'effect/phenotype;phenotype_protein;gene/protein',",
        "detail": "shepherd.preprocess",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "shepherd.pretrain",
        "description": "shepherd.pretrain",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Learn node embeddings.\")\n    # Input files/parameters\n    parser.add_argument(\"--edgelist\", type=str, default=None, help=\"File with edge list\")\n    parser.add_argument(\"--node_map\", type=str, default=None, help=\"File with node list\")\n    parser.add_argument('--save_dir', type=str, default=None, help='Directory for saving files')\n    # Tunable parameters\n    parser.add_argument('--nfeat', type=int, default=2048, help='Dimension of embedding layer')\n    parser.add_argument('--hidden', default=256, type=int)\n    parser.add_argument('--output', default=128, type=int)",
        "detail": "shepherd.pretrain",
        "documentation": {}
    },
    {
        "label": "get_dataloaders",
        "kind": 2,
        "importPath": "shepherd.pretrain",
        "description": "shepherd.pretrain",
        "peekOfCode": "def get_dataloaders(hparams, all_data):\n    print('get dataloaders')\n    train_dataloader = NeighborSampler('train', all_data.edge_index[:,all_data.train_mask], all_data.edge_index[:,all_data.train_mask], sizes = hparams['neighbor_sampler_sizes'], batch_size = hparams['batch_size'], shuffle = True, num_workers=hparams['num_workers'], do_filter_edges=hparams['filter_edges'])\n    val_dataloader = NeighborSampler('val', all_data.edge_index[:,all_data.train_mask], all_data.edge_index[:,all_data.val_mask], sizes = hparams['neighbor_sampler_sizes'], batch_size = hparams['batch_size'], shuffle = False, num_workers=hparams['num_workers'], do_filter_edges=hparams['filter_edges'])\n    test_dataloader = NeighborSampler('test', all_data.edge_index[:,all_data.train_mask], all_data.edge_index[:,all_data.test_mask], sizes = hparams['neighbor_sampler_sizes'], batch_size = hparams['batch_size'], shuffle = False, num_workers=hparams['num_workers'], do_filter_edges=hparams['filter_edges'])\n    return train_dataloader, val_dataloader, test_dataloader \ndef train(args, hparams):\n    # Seed\n    pl.seed_everything(hparams['seed'])\n    # Read input data",
        "detail": "shepherd.pretrain",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "shepherd.pretrain",
        "description": "shepherd.pretrain",
        "peekOfCode": "def train(args, hparams):\n    # Seed\n    pl.seed_everything(hparams['seed'])\n    # Read input data\n    all_data, edge_attr_dict, nodes = preprocess.preprocess_graph(args)\n    # Set up\n    if args.resume != \"\":\n        if \":\" in args.resume: # colons are not allowed in ID/resume name\n            resume_id = \"_\".join(args.resume.split(\":\"))\n        run_name = args.resume",
        "detail": "shepherd.pretrain",
        "documentation": {}
    },
    {
        "label": "save_embeddings",
        "kind": 2,
        "importPath": "shepherd.pretrain",
        "description": "shepherd.pretrain",
        "peekOfCode": "def save_embeddings(args, hparams):\n    print('Saving Embeddings')\n    # Seed\n    pl.seed_everything(hparams['seed'])\n    # Read input data\n    all_data, edge_attr_dict, nodes = preprocess.preprocess_graph(args)\n    all_data.num_nodes = len(nodes[\"node_idx\"].unique())\n    model = NodeEmbeder.load_from_checkpoint(checkpoint_path=str(Path(args.save_dir) / 'checkpoints' /  args.best_ckpt), \n                                            all_data=all_data, edge_attr_dict=edge_attr_dict, \n                                            num_nodes=len(nodes[\"node_idx\"].unique()), combined_training=False) ",
        "detail": "shepherd.pretrain",
        "documentation": {}
    },
    {
        "label": "NeighborSampler",
        "kind": 6,
        "importPath": "shepherd.samplers",
        "description": "shepherd.samplers",
        "peekOfCode": "class NeighborSampler(torch.utils.data.DataLoader):\n    r\"\"\"The neighbor sampler from the `\"Inductive Representation Learning on\n    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper, which allows\n    for mini-batch training of GNNs on large-scale graphs where full-batch\n    training is not feasible.\n    Given a GNN with :math:`L` layers and a specific mini-batch of nodes\n    :obj:`node_idx` for which we want to compute embeddings, this module\n    iteratively samples neighbors and constructs bipartite graphs that simulate\n    the actual computation flow of GNNs.\n    More specifically, :obj:`sizes` denotes how much neighbors we want to",
        "detail": "shepherd.samplers",
        "documentation": {}
    },
    {
        "label": "PatientNeighborSampler",
        "kind": 6,
        "importPath": "shepherd.samplers",
        "description": "shepherd.samplers",
        "peekOfCode": "class PatientNeighborSampler(torch.utils.data.DataLoader):\n    def __init__(self, dataset_type: str, edge_index: Union[Tensor, SparseTensor], \n                 sample_edge_index: Union[Tensor, SparseTensor],\n                 sizes: List[int], \n                 patient_dataset,\n                 all_edge_attributes,\n                 n_nodes: int,\n                 relevant_node_idx = None,\n                 do_filter_edges: Optional[bool] = False,\n                 num_nodes: Optional[int] = None, ",
        "detail": "shepherd.samplers",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Learning node embeddings.\")\n    # Input files/parameters\n    parser.add_argument(\"--edgelist\", type=str, default=None, help=\"File with edge list\")\n    parser.add_argument(\"--node_map\", type=str, default=None, help=\"File with node list\")\n    parser.add_argument('--saved_node_embeddings_path', type=str, default=None, help='Path within kg_embeddings folder to the saved KG embeddings')\n    parser.add_argument('--patient_data', default=\"disease_simulated\", type=str)\n    parser.add_argument('--run_type', choices=[\"causal_gene_discovery\", \"disease_characterization\", \"patients_like_me\"], type=str)\n    # Tunable parameters\n    parser.add_argument('--sparse_sample', default=200, type=int)",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "load_patient_datasets",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def load_patient_datasets(hparams, inference=False):\n    print('loading patient datasets')\n    if inference:\n        train_dataset = None\n        val_dataset = None\n    else:\n        train_dataset = PatientDataset(project_config.PROJECT_DIR / 'patients' / hparams['train_data'],  time=hparams['time'])\n        val_dataset = PatientDataset(project_config.PROJECT_DIR / 'patients' / hparams['validation_data'], time=hparams['time'])\n    if inference:\n        test_dataset = PatientDataset(project_config.PROJECT_DIR / 'patients' / hparams['test_data'], time=hparams['time'])",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "get_dataloaders",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def get_dataloaders(hparams, all_data, nid_to_spl_dict, n_nodes, gene_phen_dis_node_idx, train_dataset, val_dataset, test_dataset, inference=False):\n    print('Get dataloaders', flush=True)\n    shuffle = False if hparams['debug'] or inference else True\n    if not hparams['sample_from_gpd']: gene_phen_dis_node_idx = None\n    batch_sz = hparams['inference_batch_size'] if inference else hparams['batch_size']\n    sparse_sample = 1 if inference else hparams['sparse_sample']\n    #get phenotypes & genes found in train patients\n    if hparams['sample_edges_from_train_patients']:\n        phenotype_counter = Counter()\n        gene_counter = Counter()",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def get_model(args, hparams, node_hparams, all_data, edge_attr_dict, n_nodes, load_from_checkpoint=False):\n    print(\"setting up model\", hparams['model_type'])\n    # get patient model \n    if hparams['model_type'] == 'aligner':\n        if load_from_checkpoint: \n            comb_patient_model = CombinedGPAligner.load_from_checkpoint(checkpoint_path=str(Path(project_config.PROJECT_DIR /  args.best_ckpt)), \n                                    edge_attr_dict=edge_attr_dict, all_data=all_data, n_nodes=n_nodes, node_ckpt = hparams[\"saved_checkpoint_path\"], node_hparams=node_hparams)\n        else:\n            comb_patient_model = CombinedGPAligner(edge_attr_dict=edge_attr_dict, all_data=all_data, n_nodes=n_nodes, hparams=hparams, node_ckpt = hparams[\"saved_checkpoint_path\"], node_hparams=node_hparams)\n    elif hparams['model_type'] == 'patient_NCA':",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def train(args, hparams):\n    print('Training Model', flush=True)\n    # Hyperparameters\n    node_hparams = get_pretrain_hparams(args, combined=True)\n    print('Edge List: ', args.edgelist,  flush=True)\n    print('Node Map: ', args.node_map, flush=True)\n    # Set seed\n    pl.seed_everything(hparams['seed'])\n    # Read input data\n    print('Read data', flush=True)",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "def inference(args, hparams):\n    print('Running inference')\n    # Hyperparameters\n    node_hparams = get_pretrain_hparams(args, combined=True)\n    hparams.update({'add_similar_patients': False})\n    # Seed\n    pl.seed_everything(hparams['seed'])\n    # Read data\n    all_data, edge_attr_dict, nodes = preprocess.preprocess_graph(args)\n    n_nodes = len(nodes[\"node_idx\"].unique())",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_LAUNCH_BLOCKING']",
        "kind": 5,
        "importPath": "shepherd.train",
        "description": "shepherd.train",
        "peekOfCode": "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \nimport faulthandler; faulthandler.enable()\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Learning node embeddings.\")\n    # Input files/parameters\n    parser.add_argument(\"--edgelist\", type=str, default=None, help=\"File with edge list\")\n    parser.add_argument(\"--node_map\", type=str, default=None, help=\"File with node list\")\n    parser.add_argument('--saved_node_embeddings_path', type=str, default=None, help='Path within kg_embeddings folder to the saved KG embeddings')\n    parser.add_argument('--patient_data', default=\"disease_simulated\", type=str)\n    parser.add_argument('--run_type', choices=[\"causal_gene_discovery\", \"disease_characterization\", \"patients_like_me\"], type=str)",
        "detail": "shepherd.train",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "PROJECT_DIR",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "PROJECT_DIR = Path('PATH/TO/SHEPHERD')\nCURR_KG = '8.9.21_kg' \nKG_DIR = PROJECT_DIR / 'knowledge_graph' / CURR_KG\nPREDICT_RESULTS_DIR = PROJECT_DIR / 'results'\nSEED = 33\n# Modify the following variables for your dataset\nMY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "CURR_KG",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "CURR_KG = '8.9.21_kg' \nKG_DIR = PROJECT_DIR / 'knowledge_graph' / CURR_KG\nPREDICT_RESULTS_DIR = PROJECT_DIR / 'results'\nSEED = 33\n# Modify the following variables for your dataset\nMY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "KG_DIR",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "KG_DIR = PROJECT_DIR / 'knowledge_graph' / CURR_KG\nPREDICT_RESULTS_DIR = PROJECT_DIR / 'results'\nSEED = 33\n# Modify the following variables for your dataset\nMY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "PREDICT_RESULTS_DIR",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "PREDICT_RESULTS_DIR = PROJECT_DIR / 'results'\nSEED = 33\n# Modify the following variables for your dataset\nMY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "SEED",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "SEED = 33\n# Modify the following variables for your dataset\nMY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_DATA_DIR",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_DATA_DIR = Path(\"simulated_patients\")\nMY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_TRAIN_DATA",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_TRAIN_DATA = MY_DATA_DIR / f\"disease_split_train_sim_patients_{CURR_KG}.txt\"\nMY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_VAL_DATA",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_VAL_DATA = MY_DATA_DIR / f\"disease_split_val_sim_patients_{CURR_KG}.txt\"\nMY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_TEST_DATA",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_TEST_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\"\nMY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_SPL_DATA",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_SPL_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_matrix.npy)\nMY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "MY_SPL_INDEX_DATA",
        "kind": 5,
        "importPath": "project_config",
        "description": "project_config",
        "peekOfCode": "MY_SPL_INDEX_DATA = MY_DATA_DIR / \"PATH/TO/YOUR/DATA\" # Result of data_prep/shortest_paths/add_spl_to_patients.py (suffix: _spl_index_dict.pkl)",
        "detail": "project_config",
        "documentation": {}
    },
    {
        "label": "read_patients",
        "kind": 2,
        "importPath": "project_utils",
        "description": "project_utils",
        "peekOfCode": "def read_patients(filename):\n    patients = []\n    with jsonlines.open(filename) as reader:\n        for patient in reader:\n            patients.append(patient)\n    return patients\ndef write_patients(patients, filename):\n    with open(filename, \"w\") as output_file:\n        for patient_dict in patients:\n            json.dump(patient_dict, output_file)",
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "write_patients",
        "kind": 2,
        "importPath": "project_utils",
        "description": "project_utils",
        "peekOfCode": "def write_patients(patients, filename):\n    with open(filename, \"w\") as output_file:\n        for patient_dict in patients:\n            json.dump(patient_dict, output_file)\n            output_file.write('\\n')\ndef read_dicts():\n    with open(project_config.KG_DIR / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl', 'rb') as handle:\n        hpo_to_idx_dict = pickle.load(handle)\n    with open(str(project_config.KG_DIR / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'), 'rb') as handle:\n        ensembl_to_idx_dict = pickle.load(handle)",
        "detail": "project_utils",
        "documentation": {}
    },
    {
        "label": "read_dicts",
        "kind": 2,
        "importPath": "project_utils",
        "description": "project_utils",
        "peekOfCode": "def read_dicts():\n    with open(project_config.KG_DIR / f'hpo_to_idx_dict_{project_config.CURR_KG}.pkl', 'rb') as handle:\n        hpo_to_idx_dict = pickle.load(handle)\n    with open(str(project_config.KG_DIR / f'ensembl_to_idx_dict_{project_config.CURR_KG}.pkl'), 'rb') as handle:\n        ensembl_to_idx_dict = pickle.load(handle)\n    with open(project_config.KG_DIR /  f'mondo_to_idx_dict_{project_config.CURR_KG}.pkl', 'rb') as handle:\n        disease_to_idx_dict = pickle.load(handle)\n    with open(str(project_config.PROJECT_DIR / 'preprocess' / 'orphanet' / 'orphanet_to_mondo_dict.pkl'), 'rb') as handle:\n        orpha_mondo_map = pickle.load(handle)\n    return hpo_to_idx_dict, ensembl_to_idx_dict, disease_to_idx_dict, orpha_mondo_map",
        "detail": "project_utils",
        "documentation": {}
    }
]